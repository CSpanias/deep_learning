{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YyzfDxs1TPDF"
   },
   "source": [
    "# 3.6 Anatomy of a Neural Network: Understanding Core Keras APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvxrN7snTRCJ"
   },
   "source": [
    "## 3.6.1 Layers: The Building Blocks of Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WlQv72z3TXbj"
   },
   "source": [
    "The fundamental data structure in NNs is the __layer__. A layer is a data processing module that takes as input one or more tensors and that outputs one or more tensors. \n",
    "\n",
    "Some layers are __stateless__, but more __frequently layers have a state__: the layer’s __weights__, one or several tensors learned with SGD, which together contain the __network’s knowledge__.\n",
    "\n",
    "Different types of layers are appropriate for different tensor formats and different types of data processing. \n",
    "\n",
    "For instance, __simple vector data__, stored in rank-2 tensors of shape `(samples, features)`, is often processed by densely connected layers, also called fully connected or __dense layers__ (the Dense class in Keras). \n",
    "\n",
    "__Sequence data__, stored in rank-3 tensors of shape `(samples, timesteps, features)`, is typically processed by __recurrent layers__, such as an __LSTM__ layer, or __1D convolution layers__ (Conv1D). \n",
    "\n",
    "__Image data__, stored in rank-4 tensors, is usually processed by __2D convolution layers__ (Conv2D).\n",
    "\n",
    "You can think of layers as the __LEGO bricks of DL__, a metaphor that is made explicit by Keras. Building DL models in Keras is done by __clipping together compatible layers to form useful data-transformation pipelines__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vR5X4N0bUC4H"
   },
   "source": [
    "### The Base Layer Class in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arR93rOFUH35"
   },
   "source": [
    "Everything in Keras is either a `Layer` or something that closely interacts with a Layer.\n",
    "\n",
    "A `Layer` is an object that encapsulates some state (__weights__) and some computation (a __forward pass__). The weights are typically defined in a `build()` (although they could also be created in the constructor, `__init__()`), and the computation is defined in the `call()` method.\n",
    "\n",
    "In the previous chapter, we implemented a `NaiveDense` class that contained two weights `W` and `b` and applied the computation `output = activation(dot(input, W) + b)`. This is what the same layer would look like in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DipBfzzMUmhZ"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    " \n",
    "# All Keras layers inherit from the base Layer class\n",
    "class SimpleDense(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "\n",
    "    # Weight creation takes place in the build() method\n",
    "    def build(self, input_shape):\n",
    "        input_dim = input_shape[-1]\n",
    "        # add_weight() is a shortcut method for creating weights\n",
    "        # It is also possible to create standalone variables and assign them as\n",
    "        # layer attributes, like self.W = tf.Variable(tf.random.uniform(w_shape))\n",
    "        self.W = self.add_weight(shape=(input_dim, self.units),\n",
    "                                    initializer=\"random_normal\")\n",
    "        self.b = self.add_weight(shape=(self.units,),\n",
    "                             initializer=\"zeros\")\n",
    "        \n",
    "    # We define the forward pass computation in the call() method.\n",
    "    def call(self, inputs):\n",
    "        y = tf.matmul(inputs, self.W) + self.b\n",
    "        if self.activation is not None:\n",
    "          y = self.activation(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSa6fATNVx40"
   },
   "source": [
    "Once instantiated, a layer like this can be used just like a function, taking as input a TensorFlow tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KWOqtz9kV0C0",
    "outputId": "7dee60bc-8444-4d75-8c91-6ecc5bbe331e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# instantiate Layer\n",
    "my_dense = SimpleDense(units=32, activation=tf.nn.relu)\n",
    "# create some test inputs\n",
    "input_tensor = tf.ones(shape=(2, 784))\n",
    "# call the layer on the inputs, just like a function\n",
    "output_tensor = my_dense(input_tensor)\n",
    "print(output_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AebKuwjJX8UK"
   },
   "source": [
    "You’re probably wondering, why did we have to implement `call()` and `build()`, since we ended up using our layer by plainly calling it, that is to say, by using its `__call__()` method? \n",
    "\n",
    "It’s because we want to be able to create the state just in time. Let’s see how that works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPirrVWiYCof"
   },
   "source": [
    "### Automatic Shape Inference: Building Layers on the Fly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "btsW48GkYJkH"
   },
   "source": [
    "Just like with LEGO bricks, __you can only “clip” together layers that are compatible__. The notion of layer compatibility here refers specifically to the fact that __every layer will only accept input tensors of a certain shape and will return output tensors of a certain shape__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "4F0PoHZOYUEb"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "layer = layers.Dense(32, activation=\"relu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sl0HvS7aYp5a"
   },
   "source": [
    "This layer will return a tensor where the first dimension has been transformed to be 32. It can only be connected to a downstream layer that expects 32-dimensional vectors as its input.\n",
    "\n",
    "When using Keras, you don’t have to worry about size compatibility most of the time, because __the layers you add to your models are dynamically built to match the shape of the incoming layer__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "QsQY1wXzYyOC"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "\n",
    "model = models.Sequential([\n",
    "                           layers.Dense(32, activation=\"relu\"),\n",
    "                           layers.Dense(32)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YgIGuxC3ZAL8"
   },
   "source": [
    "The layers didn’t receive any information about the shape of their inputs—instead, they __automatically inferred their input shape as being the shape of the first inputs they see__.\n",
    "\n",
    "In the toy version of the Dense layer we implemented in chapter 2 (which we named NaiveDense), we had to pass the layer’s input size explicitly to the constructor in order to be able to create its weights. \n",
    "\n",
    "That’s not ideal, because it would lead to models that look like this, where each new layer needs to be made aware of the shape of the layer before it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MZgBAto4ZI1a"
   },
   "outputs": [],
   "source": [
    "# model = NaiveSequential([\n",
    "#     NaiveDense(input_size=784, output_size=32, activation=\"relu\"),\n",
    "#     NaiveDense(input_size=32, output_size=64, activation=\"relu\"),\n",
    "#     NaiveDense(input_size=64, output_size=32, activation=\"relu\"),\n",
    "#     NaiveDense(input_size=32, output_size=10, activation=\"softmax\")\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uNsQplXYZWx8"
   },
   "source": [
    "It would be even worse if the rules used by a layer to produce its output shape are complex. For instance, what if our layer returned outputs of shape `(batch, input_ size * 2 if input_size % 2 == 0 else input_size * 3)`?\n",
    "\n",
    "If we were to reimplement our `NaiveDense` layer as a Keras layer capable of automatic shape inference, it would look like the previous `SimpleDense` layer, with its `build()` and `call()` methods.\n",
    "\n",
    "In `SimpleDense`, we no longer create weights in the constructor like in the `NaiveDense example`; instead, we create them in a dedicated state-creation method, `build()`, which receives as an argument the first input shape seen by the layer. \n",
    "\n",
    "The `build()` method is called automatically the first time the layer is called (via its `__call__()` method). In fact, that’s why we defined the computation in a separate `call()` method rather than in the `__call__()` method directly. The `__call__()` method of the base layer schematically looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "oZncOm44ZwQC"
   },
   "outputs": [],
   "source": [
    "def __call__(self, inputs):\n",
    "  if not self.built:\n",
    "    self.build(inputs.shape)\n",
    "    self.built = True\n",
    "  return self.call(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nITWBi71Z7wX"
   },
   "source": [
    "With automatic shape inference, our previous example becomes simple and neat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MIRGNgbPZ9v7"
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "                          SimpleDense(32, activation=\"relu\"),\n",
    "                          SimpleDense(64, activation=\"relu\"),\n",
    "                          SimpleDense(32, activation=\"relu\"),\n",
    "                          SimpleDense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGdEVUrPaTPr"
   },
   "source": [
    "Note that __automatic shape inference__ is not the only thing that the `Layer` class’s `__call__()` method handles. It takes care of many more things, in particular __routing between eager and graph execution__ (a concept you’ll learn about in chapter 7), and __input masking__ (which we’ll cover in chapter 11). \n",
    "\n",
    "For now, just remember: __when implementing your own layers, put the forward pass in the `call()` method__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZ7HkBrWafsJ"
   },
   "source": [
    "## 3.6.2 From Layers to Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mYVHZgi8alT7"
   },
   "source": [
    "A DL model is a __graph of layers__. In Keras, that’s the `Model` class. \n",
    "\n",
    "Until now, you’ve only seen `Sequential` models (a subclass of `Model`), which are simple stacks of layers, mapping a single input to a single output. But as you move forward, you’ll be exposed to a much broader variety of __network topologies__ such as Two-branch networks, Multihead networks and Residual connections.\n",
    "\n",
    "The topology of a model defines a __hypothesis space__. You may remember that in chapter 1 we described machine learning as searching for useful representations of some input data, within a __predefined space of possibilities__, using guidance from a feedback signal. \n",
    "\n",
    "By choosing a __network topology__, you constrain your __space of possibilities__ (__hypothesis space__) to a specific series of tensor operations, mapping input data to output data. What you’ll then be searching for is a good set of values for the weight tensors involved in these tensor operations.\n",
    "\n",
    "To learn from data, you have to make assumptions about it. These assumptions define what can be learned. As such, the structure of your hypothesis space—the architecture of your model—is extremely important. It __encodes the assumptions you make about your problem__, the prior knowledge that the model starts with.\n",
    "\n",
    "Picking the right network architecture is __more an art than a science__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZ7HkBrWafsJ"
   },
   "source": [
    "## 3.6.3 The “compile” step: Configuring the learning process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model architecture is defined, you still have to choose three more things:\n",
    "\n",
    "1. __Loss function__\n",
    "2. __Optimizer__ Determines how the network will be updated based on the loss function. It implements a specific variant of SGD.\n",
    "3. __Metrics__\n",
    "\n",
    "Once you’ve picked your loss, optimizer, and metrics, you can use the built-in `compile()` and `fit()` methods to start training your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a linear classifier\n",
    "model = keras.Sequential([keras.layers.Dense(1)])\n",
    "\n",
    "# model configuration\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='mean_squared_error',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These strings are actually __shortcuts that get converted to Python objects__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.RMSprop(),\n",
    "              loss=keras.losses.MeanSquaredError(),\n",
    "              metrics=[keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is useful if you want to pass your own __custom losses or metrics__, or if you want to __further configure the objects you’re using__—for instance, by passing a `learning_rate` argument to the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-4),\n",
    "              loss=my_custom_loss,\n",
    "              metrics=[my_custom_metric_1, my_custom_metric_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6.4 Picking a Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the right loss function for the right problem is extremely important: __your network will take any shortcut it can to minimize the loss__, so if the objective doesn’t fully correlate with success for the task at hand, your network will end up doing things you may not have wanted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6.5 Understanding the `fit()` Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After `compile()` comes `fit()`. The `fit()` method implements the training loop itself. These are its key arguments:\n",
    "\n",
    "1. The __data__ (inputs and targets) to train on. It will typically be passed either in the form of __NumPy arrays__ or a __TensorFlow Dataset object__. \n",
    "1. The number of __epochs__ to train for: how many times the training loop should iterate over the data passed.\n",
    "1. The __batch size__ to use within each epoch of mini-batch gradient descent: the number of training examples considered to compute the gradients for one weight update step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    inputs,\n",
    "    targets,\n",
    "    epochs=5,\n",
    "    batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The call to `fit()` returns a `History` object. This object contains a `history` field, which is a dict mapping keys such as \"loss\" or specific metric names to the list of their per-epoch values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> history.history\n",
    "{\"binary_accuracy\": [0.855, 0.9565, 0.9555, 0.95, 0.951],\n",
    " \"loss\": [0.6573270302042366,\n",
    "          0.07434618508815766,\n",
    "          0.07687718723714351,\n",
    "          0.07412414988875389,\n",
    "          0.07617757616937161]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6.6 Monitoring Loss and Metrics on Validation Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep an eye on how the model does on new data, it’s standard practice to reserve a __subset of the training data__ as __validation data__: you won’t be training the model on this data, but you will __use it to compute a loss value and metrics value__. \n",
    "\n",
    "You do this by using the `validation_data` argument in `fit()`. Like the training data, the validation data could be passed as __NumPy arrays__ or as a __TensorFlow Dataset object__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([keras.layers.Dense(1)])\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=0.1),\n",
    "              loss=keras.losses.MeanSquaredError(),\n",
    "              metrics=[keras.metrics.BinaryAccuracy()])\n",
    "  \n",
    "indices_permutation = np.random.permutation(len(inputs))\n",
    "shuffled_inputs = inputs[indices_permutation]\n",
    "shuffled_targets = targets[indices_permutation]\n",
    " \n",
    "num_validation_samples = int(0.3 * len(inputs))\n",
    "val_inputs = shuffled_inputs[:num_validation_samples]\n",
    "val_targets = shuffled_targets[:num_validation_samples]\n",
    "training_inputs = shuffled_inputs[num_validation_samples:]\n",
    "training_targets = shuffled_targets[num_validation_samples:]\n",
    "\n",
    "model.fit(\n",
    "    training_inputs,\n",
    "    training_targets,\n",
    "    epochs=5,\n",
    "    batch_size=16,\n",
    "    validation_data=(val_inputs, val_targets)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of the loss on the validation data is called the __validation loss__, to distinguish it from the __training loss__. \n",
    "\n",
    "Note that if you want to compute the validation loss and metrics after the training is complete, you can call the `evaluate()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_and_metrics = model.evaluate(val_inputs, val_targets, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`evaluate()` will iterate in batches (of size `batch_size`) over the data passed and return a list of scalars, where the first entry is the validation loss and the following entries are the validation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6.7 Inference: Using a model after training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you’ve trained your model, you’re going to want to use it to make predictions on new data. This is called __inference__. \n",
    "\n",
    "To do this, a naive approach would simply be to `__call__()` the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(new_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this will process all inputs in `new_inputs` at once, which may not be feasible if you’re looking at a lot of data (in particular, it may require more memory than your GPU has).\n",
    "\n",
    "A better way to do inference is to use the `predict()` method. It will iterate over the data in small batches and return a NumPy array of predictions. And unlike `__call__()`, it can also process TensorFlow Dataset objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(new_inputs, batch_size=128)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "chapter3_6_KerasAPI.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
