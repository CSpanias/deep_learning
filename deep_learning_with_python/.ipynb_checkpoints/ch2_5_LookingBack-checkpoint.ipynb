{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50e8197e",
   "metadata": {},
   "source": [
    "# Deep Learning with Python (2nd ed.)\n",
    "[Website](https://www.manning.com/books/deep-learning-with-python-second-edition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec507c0",
   "metadata": {},
   "source": [
    "# 2.5 Looking Back at Our First Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59066cdc",
   "metadata": {},
   "source": [
    "## 5.1 Reimplementing our first example from scratch in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037efa48",
   "metadata": {},
   "source": [
    "### A Simple Dense Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961181ec",
   "metadata": {},
   "source": [
    "You’ve learned earlier that the Dense layer implements the following input transformation, where `W` and `b` are model parameters, and activation is an element-wise function (usually `relu`, but it would be `softmax` for the last layer).\n",
    "\n",
    "`output = activation(dot(W, input) + b)`\n",
    "\n",
    "Let’s implement a simple Python class, `NaiveDense`, that creates two TensorFlow variables, `W` and `b`, and exposes a `__call__()` method that applies the preceding transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7103525e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "  \n",
    "class NaiveDense:\n",
    "    def __init__(self, input_size, output_size, activation):\n",
    "        self.activation = activation\n",
    "         \n",
    "        # Create a matrix, W, of shape (input_size, output_size)\n",
    "        w_shape = (input_size, output_size)\n",
    "        # initialize with random values\n",
    "        w_initial_value = tf.random.uniform(w_shape, minval=0, maxval=1e-1)\n",
    "        self.W = tf.Variable(w_initial_value)\n",
    "        # Create a vector, b, of shape (output_size,)\n",
    "        b_shape = (output_size,)\n",
    "        # initialize with zeros\n",
    "        b_initial_value = tf.zeros(b_shape)\n",
    "        self.b = tf.Variable(b_initial_value)\n",
    "        \n",
    "    # Apply the forward pass\n",
    "    def __call__(self, inputs):\n",
    "        return self.activation(tf.matmul(inputs, self.W) + self.b)\n",
    "    \n",
    "    # Convenience method for retrieving the layer’s weights\n",
    "    @property\n",
    "    def weights(self):\n",
    "        return [self.W, self.b]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a406a610",
   "metadata": {},
   "source": [
    "### A Simple Sequential Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ce5cd2",
   "metadata": {},
   "source": [
    "Now, let’s create a `NaiveSequential` class to chain these layers. It wraps a list of layers and exposes a `__call__()` method that simply calls the underlying layers on the inputs, in order. It also features a weights property to easily keep track of the layers’ parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21d6c83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveSequential:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "  \n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.layers:\n",
    "           x = layer(x)\n",
    "        return x\n",
    "  \n",
    "    @property \n",
    "    def weights(self):\n",
    "       weights = []\n",
    "       for layer in self.layers:\n",
    "           weights += layer.weights\n",
    "       return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de10b19",
   "metadata": {},
   "source": [
    "Using this `NaiveDense` class and this `NaiveSequential` class, we can create a mock Keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eae77ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NaiveSequential([\n",
    "    NaiveDense(input_size=28 * 28, output_size=512, activation=tf.nn.relu),\n",
    "    NaiveDense(input_size=512, output_size=10, activation=tf.nn.softmax)\n",
    "]) \n",
    "assert len(model.weights) == 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2f3577",
   "metadata": {},
   "source": [
    "### A Batch Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b00f75",
   "metadata": {},
   "source": [
    "Next, we need a way to iterate over the MNIST data in mini-batches. This is easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "853fe412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "  \n",
    "class BatchGenerator:\n",
    "    def __init__(self, images, labels, batch_size=128):\n",
    "        assert len(images) == len(labels)\n",
    "        self.index = 0\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = math.ceil(len(images) / batch_size)\n",
    " \n",
    "    def next(self):\n",
    "        images = self.images[self.index : self.index + self.batch_size]\n",
    "        labels = self.labels[self.index : self.index + self.batch_size]\n",
    "        self.index += self.batch_size\n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaf0e94",
   "metadata": {},
   "source": [
    "## 5.2 Running one training step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0d740a",
   "metadata": {},
   "source": [
    "The most difficult part of the process is the “training step”: updating the weights of the model after running it on one batch of data. We need to\n",
    "\n",
    "1. Compute the predictions of the model for the images in the batch.\n",
    "2. Compute the loss value for these predictions, given the actual labels.\n",
    "3. Compute the gradient of the loss with regard to the model’s weights.\n",
    "4. Move the weights by a small amount in the direction opposite to the gradient.\n",
    "\n",
    "To compute the gradient, we will use the TensorFlow `GradientTape` object we introduced in section 2.4.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8966fac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_training_step(model, images_batch, labels_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images_batch)\n",
    "        per_sample_losses = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "            labels_batch, predictions)\n",
    "        average_loss = tf.reduce_mean(per_sample_losses)\n",
    "    gradients = tape.gradient(average_loss, model.weights)\n",
    "    update_weights(gradients, model.weights)\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc7b0e1",
   "metadata": {},
   "source": [
    "As you already know, the purpose of the “weight update” step (represented by the preceding update_weights function) is to move the weights by “a bit” in a direction that will reduce the loss on this batch. \n",
    "\n",
    "The magnitude of the move is determined by the “learning rate,” typically a small quantity. \n",
    "\n",
    "The simplest way to implement this update_weights function is to subtract `gradient * learning_rate` from each weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbd910b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3 \n",
    "  \n",
    "def update_weights(gradients, weights):\n",
    "    for g, w in zip(gradients, weights):\n",
    "        w.assign_sub(g * learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b93dd84",
   "metadata": {},
   "source": [
    "In practice, you would almost never implement a weight update step like this by hand. Instead, you would use an `Optimizer` instance from Keras, like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc6cc238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "  \n",
    "optimizer = optimizers.SGD(learning_rate=1e-3)\n",
    "  \n",
    "def update_weights(gradients, weights):\n",
    "    optimizer.apply_gradients(zip(gradients, weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fcce83",
   "metadata": {},
   "source": [
    "Now that our per-batch training step is ready, we can move on to implementing an entire epoch of training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f9d7fb",
   "metadata": {},
   "source": [
    "## 5.3 The Full Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594c5e25",
   "metadata": {},
   "source": [
    "An epoch of training simply consists of repeating the training step for each batch in the training data, and the full training loop is simply the repetition of one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbdb0c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, images, labels, epochs, batch_size=128):\n",
    "    for epoch_counter in range(epochs):\n",
    "        print(f\"Epoch {epoch_counter}\")\n",
    "        batch_generator = BatchGenerator(images, labels)\n",
    "        for batch_counter in range(batch_generator.num_batches):\n",
    "            images_batch, labels_batch = batch_generator.next()\n",
    "            loss = one_training_step(model, images_batch, labels_batch)\n",
    "            if batch_counter % 100 == 0:\n",
    "                print(f\"loss at batch {batch_counter}: {loss:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc52efc3",
   "metadata": {},
   "source": [
    "Let’s test drive it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f39fb055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "loss at batch 0: 4.41\n",
      "loss at batch 100: 2.21\n",
      "loss at batch 200: 2.19\n",
      "loss at batch 300: 2.11\n",
      "loss at batch 400: 2.23\n",
      "Epoch 1\n",
      "loss at batch 0: 1.91\n",
      "loss at batch 100: 1.86\n",
      "loss at batch 200: 1.82\n",
      "loss at batch 300: 1.73\n",
      "loss at batch 400: 1.85\n",
      "Epoch 2\n",
      "loss at batch 0: 1.60\n",
      "loss at batch 100: 1.57\n",
      "loss at batch 200: 1.50\n",
      "loss at batch 300: 1.44\n",
      "loss at batch 400: 1.53\n",
      "Epoch 3\n",
      "loss at batch 0: 1.34\n",
      "loss at batch 100: 1.33\n",
      "loss at batch 200: 1.24\n",
      "loss at batch 300: 1.22\n",
      "loss at batch 400: 1.30\n",
      "Epoch 4\n",
      "loss at batch 0: 1.14\n",
      "loss at batch 100: 1.15\n",
      "loss at batch 200: 1.05\n",
      "loss at batch 300: 1.06\n",
      "loss at batch 400: 1.13\n",
      "Epoch 5\n",
      "loss at batch 0: 1.00\n",
      "loss at batch 100: 1.01\n",
      "loss at batch 200: 0.91\n",
      "loss at batch 300: 0.94\n",
      "loss at batch 400: 1.01\n",
      "Epoch 6\n",
      "loss at batch 0: 0.89\n",
      "loss at batch 100: 0.90\n",
      "loss at batch 200: 0.81\n",
      "loss at batch 300: 0.85\n",
      "loss at batch 400: 0.92\n",
      "Epoch 7\n",
      "loss at batch 0: 0.80\n",
      "loss at batch 100: 0.81\n",
      "loss at batch 200: 0.73\n",
      "loss at batch 300: 0.77\n",
      "loss at batch 400: 0.85\n",
      "Epoch 8\n",
      "loss at batch 0: 0.74\n",
      "loss at batch 100: 0.75\n",
      "loss at batch 200: 0.66\n",
      "loss at batch 300: 0.72\n",
      "loss at batch 400: 0.79\n",
      "Epoch 9\n",
      "loss at batch 0: 0.69\n",
      "loss at batch 100: 0.69\n",
      "loss at batch 200: 0.61\n",
      "loss at batch 300: 0.67\n",
      "loss at batch 400: 0.75\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "  \n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype(\"float32\") / 255  \n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype(\"float32\") / 255 \n",
    "  \n",
    "fit(model, train_images, train_labels, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f66511",
   "metadata": {},
   "source": [
    "## 5.4 Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e882b1",
   "metadata": {},
   "source": [
    "We can evaluate the model by taking the argmax of its predictions over the test images, and comparing it to the expected labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50f8074a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "predictions = model(test_images)\n",
    "predictions = predictions.numpy()\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "matches = predicted_labels == test_labels\n",
    "print(f\"accuracy: {matches.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2c5afe",
   "metadata": {},
   "source": [
    "All done! As you can see, it’s quite a bit of work to do “by hand” what you can do in a few lines of Keras code. But because you’ve gone through these steps, you should now have a crystal clear understanding of what goes on inside a neural network when you call `fit()`. Having this low-level mental model of what your code is doing behind the scenes will make you better able to leverage the high-level features of the Keras API."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
