{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chapter3_6_KerasAPI.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNuNrH/MN1/BhtV29IlFL20",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CSpanias/deep_learning/blob/main/chapter3_6_KerasAPI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNTbWumfS_Kw"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.6 Anatomy of a Neural Network: Understanding Core Keras APIs"
      ],
      "metadata": {
        "id": "YyzfDxs1TPDF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.6.1 Layers: The Building Blocks of Deep Learning"
      ],
      "metadata": {
        "id": "xvxrN7snTRCJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The fundamental data structure in NNs is the __layer__. A layer is a data processing module that takes as input one or more tensors and that outputs one or more tensors. \n",
        "\n",
        "Some layers are __stateless__, but more __frequently layers have a state__: the layer’s __weights__, one or several tensors learned with stochastic gradient descent, which together contain the __network’s knowledge__.\n",
        "\n",
        "Different types of layers are appropriate for different tensor formats and different types of data processing. \n",
        "\n",
        "For instance, __simple vector data__, stored in rank-2 tensors of shape `(samples, features)`, is often processed by densely connected layers, also called fully connected or __dense layers__ (the Dense class in Keras). \n",
        "\n",
        "__Sequence data__, stored in rank-3 tensors of shape `(samples, timesteps, features)`, is typically processed by __recurrent layers__, such as an LSTM layer, or __1D convolution layers__ (Conv1D). \n",
        "\n",
        "__Image data__, stored in rank-4 tensors, is usually processed by __2D convolution layers__ (Conv2D).\n",
        "\n",
        "You can think of layers as the __LEGO bricks of deep learning__, a metaphor that is made explicit by Keras. Building deep learning models in Keras is done by __clipping together compatible layers to form useful data-transformation pipelines__."
      ],
      "metadata": {
        "id": "WlQv72z3TXbj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Base Layer Class in Keras"
      ],
      "metadata": {
        "id": "vR5X4N0bUC4H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A simple API should have a single abstraction around which everything is centered. In Keras, that’s the `Layer` class. Everything in Keras is either a `Layer` or something that closely interacts with a Layer.\n",
        "\n",
        "A `Layer` is an object that encapsulates some state (weights) and some computation (a forward pass). The weights are typically defined in a `build()` (although they could also be created in the constructor, `__init__()`), and the computation is defined in the `call()` method.\n",
        "\n",
        "In the previous chapter, we implemented a `NaiveDense` class that contained two weights `W` and `b` and applied the computation `output = activation(dot(input, W) + b)`. This is what the same layer would look like in Keras."
      ],
      "metadata": {
        "id": "arR93rOFUH35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        " \n",
        "# All Keras layers inherit from the base Layer class\n",
        "class SimpleDense(keras.layers.Layer):\n",
        "  def __init__(self, units, activation=None):\n",
        "    super().__init__()\n",
        "    self.units = units\n",
        "    self.activation = activation\n",
        "\n",
        "  # Weight creation takes place in the build() method\n",
        "  def build(self, input_shape):\n",
        "    input_dim = input_shape[-1]\n",
        "    # add_weight() is a shortcut method for creating weights\n",
        "    # It is also possible to create standalone variables and assign them as\n",
        "    # layer attributes, like self.W = tf.Variable(tf.random.uniform(w_shape))\n",
        "    self.W = self.add_weight(shape=(input_dim, self.units),\n",
        "                                    initializer=\"random_normal\")\n",
        "    self.b = self.add_weight(shape=(self.units,),\n",
        "                             initializer=\"zeros\")\n",
        "    \n",
        "  def call(self, inputs):\n",
        "    y = tf.matmul(inputs, self.W) + self.b\n",
        "    if self.activation is not None:\n",
        "      y = self.activation(y)\n",
        "    return y"
      ],
      "metadata": {
        "id": "DipBfzzMUmhZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once instantiated, a layer like this can be used just like a function, taking as input a TensorFlow tensor."
      ],
      "metadata": {
        "id": "qSa6fATNVx40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# instantiate Layer\n",
        "my_dense = SimpleDense(units=32, activation=tf.nn.relu)\n",
        "# create some test inputs\n",
        "input_tensor = tf.ones(shape=(2, 784))\n",
        "# call the layer on the inputs, just like a function\n",
        "output_tensor = my_dense(input_tensor)\n",
        "print(output_tensor.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWOqtz9kV0C0",
        "outputId": "7dee60bc-8444-4d75-8c91-6ecc5bbe331e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You’re probably wondering, why did we have to implement `call()` and `build()`, since we ended up using our layer by plainly calling it, that is to say, by using its `__call__()` method? \n",
        "\n",
        "It’s because we want to be able to create the state just in time. Let’s see how that works."
      ],
      "metadata": {
        "id": "AebKuwjJX8UK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Automatic Shape Inference: Building Layers on the Fly"
      ],
      "metadata": {
        "id": "YPirrVWiYCof"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just like with LEGO bricks, __you can only “clip” together layers that are compatible__. The notion of layer compatibility here refers specifically to the fact that __every layer will only accept input tensors of a certain shape and will return output tensors of a certain shape__."
      ],
      "metadata": {
        "id": "btsW48GkYJkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "layer = layers.Dense(32, activation=\"relu\")"
      ],
      "metadata": {
        "id": "4F0PoHZOYUEb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This layer will return a tensor where the first dimension has been transformed to be 32. It can only be connected to a downstream layer that expects 32-dimensional vectors as its input.\n",
        "\n",
        "When using Keras, you don’t have to worry about size compatibility most of the time, because __the layers you add to your models are dynamically built to match the shape of the incoming layer__."
      ],
      "metadata": {
        "id": "Sl0HvS7aYp5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models\n",
        "\n",
        "model = models.Sequential([\n",
        "                           layers.Dense(32, activation=\"relu\"),\n",
        "                           layers.Dense(32)\n",
        "])"
      ],
      "metadata": {
        "id": "QsQY1wXzYyOC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The layers didn’t receive any information about the shape of their inputs—instead, they __automatically inferred their input shape as being the shape of the first inputs they see__.\n",
        "\n",
        "In the toy version of the Dense layer we implemented in chapter 2 (which we named NaiveDense), we had to pass the layer’s input size explicitly to the constructor in order to be able to create its weights. \n",
        "\n",
        "That’s not ideal, because it would lead to models that look like this, where each new layer needs to be made aware of the shape of the layer before it."
      ],
      "metadata": {
        "id": "YgIGuxC3ZAL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model = NaiveSequential([\n",
        "#     NaiveDense(input_size=784, output_size=32, activation=\"relu\"),\n",
        "#     NaiveDense(input_size=32, output_size=64, activation=\"relu\"),\n",
        "#     NaiveDense(input_size=64, output_size=32, activation=\"relu\"),\n",
        "#     NaiveDense(input_size=32, output_size=10, activation=\"softmax\")\n",
        "# ])"
      ],
      "metadata": {
        "id": "MZgBAto4ZI1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It would be even worse if the rules used by a layer to produce its output shape are complex. For instance, what if our layer returned outputs of shape `(batch, input_ size * 2 if input_size % 2 == 0 else input_size * 3)`?\n",
        "\n",
        "If we were to reimplement our `NaiveDense` layer as a Keras layer capable of automatic shape inference, it would look like the previous `SimpleDense` layer, with its `build()` and `call()` methods.\n",
        "\n",
        "In `SimpleDense`, we no longer create weights in the constructor like in the `NaiveDense example`; instead, we create them in a dedicated state-creation method, `build()`, which receives as an argument the first input shape seen by the layer. \n",
        "\n",
        "The `build()` method is called automatically the first time the layer is called (via its `__call__()` method). In fact, that’s why we defined the computation in a separate `call()` method rather than in the `__call__()` method directly. The `__call__()` method of the base layer schematically looks like this:"
      ],
      "metadata": {
        "id": "uNsQplXYZWx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def __call__(self, inputs):\n",
        "  if not self.built:\n",
        "    self.build(inputs.shape)\n",
        "    self.built = True\n",
        "  return self.call(inputs)"
      ],
      "metadata": {
        "id": "oZncOm44ZwQC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With automatic shape inference, our previous example becomes simple and neat."
      ],
      "metadata": {
        "id": "nITWBi71Z7wX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "                          SimpleDense(32, activation=\"relu\"),\n",
        "                          SimpleDense(64, activation=\"relu\"),\n",
        "                          SimpleDense(32, activation=\"relu\"),\n",
        "                          SimpleDense(10, activation=\"softmax\")\n",
        "])"
      ],
      "metadata": {
        "id": "MIRGNgbPZ9v7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that __automatic shape inference__ is not the only thing that the `Layer` class’s `__call__()` method handles. It takes care of many more things, in particular __routing between eager and graph execution__ (a concept you’ll learn about in chapter 7), and __input masking__ (which we’ll cover in chapter 11). \n",
        "\n",
        "For now, just remember: __when implementing your own layers, put the forward pass in the `call()` method__."
      ],
      "metadata": {
        "id": "rGdEVUrPaTPr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.6 From Layers to Models"
      ],
      "metadata": {
        "id": "RZ7HkBrWafsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mYVHZgi8alT7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}