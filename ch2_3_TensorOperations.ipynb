{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a31d1ef8",
   "metadata": {},
   "source": [
    "# Deep Learning with Python (2nd ed.)\n",
    "[Website](https://www.manning.com/books/deep-learning-with-python-second-edition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3826a3",
   "metadata": {},
   "source": [
    "# 2.3 Tensor Operations\n",
    "1. [Element-Wise Operations](#elementWiseOperations)\n",
    "2. [Broadcasting](#broadcasting)\n",
    "3. [Tensor Product](#tensorProduct)\n",
    "4. [Tensor Reshaping](#tensorReshaping)\n",
    "5. [Geometric Interpretation of Tensor Operations](#geometricInterpretation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee0e854",
   "metadata": {},
   "source": [
    "Much as __any computer program can be ultimately reduced to a small set of binary operations on binary inputs__ (AND, OR, NOR, and so on), all __transformations learned by deep NNs can be reduced to a handful of tensor operations__ (or tensor functions) applied to tensors of numeric data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcec5c43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x18330cdc6a0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# a keras instance\n",
    "keras.layers.Dense(512, activation='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984d7208",
   "metadata": {},
   "source": [
    "This layer can be interpreted as a __function__, which __takes as input a matrix and returns another matrix__—a new representation for the input tensor. \n",
    "\n",
    "Specifically, the function is as follows (where `W` is a matrix and `b` is a vector, both attributes of the layer):\n",
    "\n",
    "`output = relu(dot(input, W) + b)`\n",
    "\n",
    "We have __three tensor operations__ here:\n",
    "\n",
    "1. A dot product (dot) between the input tensor and a tensor named `W`\n",
    "2. An addition (+) between the resulting matrix and a vector `b`\n",
    "3. A relu (rectified linear unit) operation: relu(x) is max(x, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b231421c",
   "metadata": {},
   "source": [
    "<a name=\"elementWiseOperations\"></a>\n",
    "## 2.3.1 Element-Wise Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e88c195",
   "metadata": {},
   "source": [
    "The __relu operation__ and __addition__ are __element-wise operations__: operations that are __applied independently to each entry in the tensors being considered__. \n",
    "\n",
    "This means these operations are highly amenable to __massively parallel implementations__ (__vectorized implementations__). \n",
    "\n",
    "The code below demonstrates a naive Python implementation of an element-wise operation of the relu and addition operations, using a __`for` loop__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24ba67d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relu operation\n",
    "def naive_relu(x):\n",
    "    # check that x is a rank-2 tensor\n",
    "    assert len(x.shape) == 2\n",
    "    # avoid overwriting the input tensor\n",
    "    x = x.copy()\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] = max(x[i, j], 0)\n",
    "    return x\n",
    "\n",
    "# addition\n",
    "def naive_add(x, y):\n",
    "    # check that x is a rank-2 tensor\n",
    "    assert len(x.shape) == 2\n",
    "    # check that y is a rank-2 tensor\n",
    "    assert x.shape == y.shape\n",
    "    # avoid overwriting the input tensor\n",
    "    x = x.copy()\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] += y[i, j]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cf6031",
   "metadata": {},
   "source": [
    "These operations are available as __well-optimized built-in NumPy functions__, which themselves delegate the heavy lifting to a __Basic Linear Algebra Subprograms__ (BLAS) implementation. \n",
    "\n",
    "BLAS are __low-level, highly parallel, efficient tensor-manipulation routines that are typically implemented in Fortran or C__.\n",
    "\n",
    "So, in NumPy, you can do the following element-wise operation, and it will be blazing fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04521b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "z = x + y\n",
    "z = np.maximum(z, 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca273254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 0.01 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "  \n",
    "x = np.random.random((20, 100))\n",
    "y = np.random.random((20, 100))\n",
    "  \n",
    "t0 = time.time() \n",
    "for _ in range(1000):\n",
    "    # addition\n",
    "    z = x + y\n",
    "    # relu\n",
    "    z = np.maximum(z, 0.) \n",
    "print(\"Took: {0:.2f} s\".format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89fd465d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 2.81 s\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time() \n",
    "for _ in range(1000):\n",
    "    # addition\n",
    "    z = naive_add(x, y)\n",
    "    # relu\n",
    "    z = naive_relu(z) \n",
    "print(\"Took: {0:.2f} s\".format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de489b4d",
   "metadata": {},
   "source": [
    "Likewise, when running TensorFlow code on a __GPU__, element-wise operations are __executed via fully vectorized CUDA implementations__ that can best utilize the highly parallel GPU chip architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a33ee62",
   "metadata": {},
   "source": [
    "<a name=\"broadcasting\"></a>\n",
    "## 2.3.2 Broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d199f725",
   "metadata": {},
   "source": [
    "Our earlier naive implementation of naive_add only supports the addition of __rank-2 tensors with identical shapes__. \n",
    "\n",
    "But in the Dense layer introduced earlier, we added a __rank-2 tensor with a vector__. What happens with addition when the shapes of the two tensors being added differ?\n",
    "\n",
    "When possible, and if there’s no ambiguity, __the smaller tensor will be broadcast to match the shape of the larger tensor__. Broadcasting consists of two steps:\n",
    "\n",
    "1. Axes (called __broadcast axes__) are added to the smaller tensor to match the ndim of the larger tensor.\n",
    "2. The smaller tensor is __repeated alongside these new axes__ to match the full shape of the larger tensor.\n",
    "\n",
    "\n",
    "Consider `X` with shape `(32, 10)` and `y` with shape `(10,)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80f8dd16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 10), (10,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.random((32, 10))\n",
    "y = np.random.random((10,))\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d50fbca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add an empty first axis to y\n",
    "y = np.expand_dims(y, axis=0)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa9ccbba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repeat y 32 times alongside new axis\n",
    "Y = np.concatenate([y] * 32, axis=0)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d55dbb",
   "metadata": {},
   "source": [
    "In terms of implementation, __no new rank-2 tensor is created__, because that would be terribly inefficient. \n",
    "\n",
    "__The repetition operation is entirely virtual__: it happens at the algorithmic level rather than at the memory level. \n",
    "\n",
    "But thinking of the vector being repeated 10 times alongside a new axis is a helpful mental model. \n",
    "\n",
    "Here’s what a naive implementation would look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eda53075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_add_matrix_and_vector(x, y):\n",
    "    # check that x is a rank-2 tensor\n",
    "    assert len(x.shape) == 2\n",
    "    # check that y is a vector (rank-1 tensor)\n",
    "    assert len(y.shape) == 1\n",
    "    #  # the first dim of x must be equal to the 0th dim of y\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    # avoid overwriting the input tensor\n",
    "    x = x.copy()\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] += y[j]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841ae761",
   "metadata": {},
   "source": [
    "With broadcasting, you can generally perform element-wise operations that take two inputs tensors if one tensor has shape `(a, b, ... n, n + 1, ... m)` and the other has shape `(n, n + 1, ... m)`. \n",
    "\n",
    "The broadcasting will then automatically happen for axes a through `n - 1`.\n",
    "\n",
    "The following example applies the element-wise maximum operation to two tensors of different shapes via broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62433819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: (64, 3, 32, 10) -> rank-4 tensor\n",
      "y.shape: (32, 10) -> matrix (rank-2 tensor)\n",
      "z.shape: (64, 3, 32, 10) -> rank-4 tensor\n"
     ]
    }
   ],
   "source": [
    "x = np.random.random((64, 3, 32, 10))\n",
    "print(f\"x.shape: {x.shape} -> rank-4 tensor\")\n",
    "y = np.random.random(((32,10)))\n",
    "print(f\"y.shape: {y.shape} -> matrix (rank-2 tensor)\")\n",
    "z = np.maximum(x, y)\n",
    "print(f\"z.shape: {z.shape} -> rank-4 tensor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f108bc",
   "metadata": {},
   "source": [
    "<a name=\"tensorProduct\"></a>\n",
    "## 2.3.3 Tensor Product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a6b509",
   "metadata": {},
   "source": [
    "The tensor product, or __dot product__ (not to be confused with an element-wise product, the `*` operator), is __one of the most common, most useful tensor operations__.\n",
    "\n",
    "In NumPy, a tensor product is done using the `np.dot` function (because the mathematical notation for tensor product is usually a dot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0899c181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: (32,) -> vector (rank-1 tensor)\n",
      "y.shape: (32,) -> vector (rank-1 tensor)\n",
      "z.shape: () -> scalar!\n"
     ]
    }
   ],
   "source": [
    "x = np.random.random((32,))\n",
    "print(f\"x.shape: {x.shape} -> vector (rank-1 tensor)\")\n",
    "y = np.random.random((32,))\n",
    "print(f\"y.shape: {y.shape} -> vector (rank-1 tensor)\")\n",
    "z = np.dot(x, y)\n",
    "print(f\"z.shape: {z.shape} -> scalar!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cd3f7c",
   "metadata": {},
   "source": [
    "Mathematically, what does the dot operation do? \n",
    "\n",
    "Let’s start with the dot product of two vectors, `x` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "241a5825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.824177458558538"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def naive_vector_dot(x, y):\n",
    "    # check that x is a rank-1 tensor (vector)\n",
    "    assert len(x.shape) == 1\n",
    "     # check that y is a rank-1 tensor (vector)\n",
    "    assert len(y.shape) == 1\n",
    "     # check that they have equal dims\n",
    "    assert x.shape[0] == y.shape[0]\n",
    "    z = 0.\n",
    "    for i in range(x.shape[0]):\n",
    "        z += x[i] * y[i]\n",
    "    return z\n",
    "\n",
    "naive_vector_dot(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2e7263",
   "metadata": {},
   "source": [
    "You’ll have noticed that __the dot product between two vectors is a scalar__ and that only __vectors with the same number of elements__ are compatible for a dot product.\n",
    "\n",
    "You can also take the __dot product between a matrix `x` and a vector `y`__, which __returns a vector where the coefficients are the dot products between `y` and the rows of `x`__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bedb9a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_matrix_vector_dot(x, y):\n",
    "    # check that x is a rank-2 tensor (matrix)\n",
    "    assert len(x.shape) == 2\n",
    "    # check that y is a rank-1 tensor (vector)\n",
    "    assert len(y.shape) == 1\n",
    "    # the first dim of x must be equal to the 0th dim of y\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    # this operation returns a vector of 0s with the same shape as y\n",
    "    z = np.zeros(x.shape[0])\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            z[i] += x[i, j] * y[j]\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7208cfeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: (32, 10) -> matrix (rank-2 tensor)\n",
      "y.shape: (10,) -> vector (rank-1 tensor)\n",
      "z.shape: (32,) -> vector (rank-1 tensor)\n"
     ]
    }
   ],
   "source": [
    "x = np.random.random((32, 10))\n",
    "print(f\"x.shape: {x.shape} -> matrix (rank-2 tensor)\")\n",
    "y = np.random.random((10,))\n",
    "print(f\"y.shape: {y.shape} -> vector (rank-1 tensor)\")\n",
    "\n",
    "z = naive_matrix_vector_dot(x, y)\n",
    "print(f\"z.shape: {z.shape} -> vector (rank-1 tensor)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ef57ef",
   "metadata": {},
   "source": [
    "You could also reuse the code we wrote previously, which highlights the __relationship between a matrix-vector product and a vector product__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0cec4723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_matrix_vector_dot(x, y):\n",
    "    z = np.zeros(x.shape[0])\n",
    "    for i in range(x.shape[0]):\n",
    "        z[i] = naive_vector_dot(x[i, :], y)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00ee8e5",
   "metadata": {},
   "source": [
    "Note that as soon as one of the two tensors has an `ndim` greater than 1, __dot is no longer symmetric__, which is to say that __`dot(x, y)` isn’t the same as `dot(y, x)`__.\n",
    "\n",
    "Of course, a dot product generalizes to tensors with an arbitrary number of axes. __The most common applications may be the dot product between two matrices__. \n",
    "\n",
    "You can take the dot product of two matrices `x` and `y` `(dot(x, y))` if and only if `x.shape[1] == y.shape[0]`. \n",
    "\n",
    "The result is a matrix with shape `(x.shape[0], y.shape[1])`, where the coefficients are the vector products between the rows of `x` and the columns of `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4ebdc806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_matrix_dot(x, y):\n",
    "    # check that x is a rank-2 tensor (matrix)\n",
    "    assert len(x.shape) == 2\n",
    "    # check that y is a rank-2 tensor (matrix)\n",
    "    assert len(y.shape) == 2\n",
    "    # the first dim of x must be equal to the 0th dim of y\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    # this operation returns a matrix of 0s with a specific shape\n",
    "    z = np.zeros((x.shape[0], y.shape[1]))\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            row_x = x[i, :]\n",
    "            column_y = y[:, j]\n",
    "            z[i, j] = naive_vector_dot(row_x, column_y)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77baa14d",
   "metadata": {},
   "source": [
    "To understand __dot-product shape compatibility__, it helps to visualize the input and output tensors by aligning them as shown below.\n",
    "\n",
    "![](https://drek4537l1klr.cloudfront.net/chollet2/Figures/02-05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7122bf8c",
   "metadata": {},
   "source": [
    "In the figure, `x`, `y`, and `z` are pictured as rectangles (literal boxes of coefficients). \n",
    "\n",
    "Because __the rows of `x` and the columns of `y` must have the same size__, it follows that __the width of `x` must match the height of `y`__. \n",
    "\n",
    "If you go on to develop new machine learning algorithms, you’ll likely be drawing such diagrams often.\n",
    "\n",
    "More generally, __you can take the dot product between higher-dimensional tensors__, following the same rules for shape compatibility as outlined earlier for the 2D case.\n",
    "\n",
    "`(a, b, c, d) • (d,) → (a, b, c)` <br>\n",
    "`(a, b, c, d) • (d, e) → (a, b, c, e)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c6c8c7",
   "metadata": {},
   "source": [
    "<a name=\"tensorReshaping\"></a>\n",
    "## 2.3.4 Tensor Reshaping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02b0fcf",
   "metadata": {},
   "source": [
    "A third type of tensor operation that’s essential to understand is __tensor reshaping__. \n",
    "\n",
    "Although it wasn’t used in the Dense layers in our first NN example, __we used it when we preprocessed the digits data before feeding it into our model__.\n",
    "\n",
    "`train_images = train_images.reshape((60000, 28 * 28))`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a223e70",
   "metadata": {},
   "source": [
    "Reshaping a tensor means __rearranging its rows and columns to match a target shape__. \n",
    "\n",
    "Naturally, __the reshaped tensor has the same total number of coefficients as the initial tensor__. \n",
    "\n",
    "Reshaping is best understood via simple examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "54e87c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: (3, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [2., 3.],\n",
       "       [4., 5.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[0., 1.],\n",
    "              [2., 3.],\n",
    "              [4., 5.]])\n",
    "              \n",
    "print(f\"x.shape: {x.shape}\")\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b7da85e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: (6, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [3.],\n",
       "       [4.],\n",
       "       [5.]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.reshape((6, 1))\n",
    "print(f\"x.shape: {x.shape}\")\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "049a211e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: (2, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 2.],\n",
       "       [3., 4., 5.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.reshape((2, 3))\n",
    "print(f\"x.shape: {x.shape}\")\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85858f61",
   "metadata": {},
   "source": [
    "A special case of reshaping that’s commonly encountered is __transposition__. \n",
    "\n",
    "Transposing a matrix means __exchanging its rows and its columns__, so that `x[i, :]` becomes `x[:, i]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5412f180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: (300, 20)\n",
      "x transposed: (20, 300)\n"
     ]
    }
   ],
   "source": [
    "x = np.zeros((300, 20))\n",
    "print(f\"x.shape: {x.shape}\")\n",
    "x = np.transpose(x)\n",
    "print(f\"x transposed: {x.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae18c5e",
   "metadata": {},
   "source": [
    "<a name=\"geometricInterpretation\"></a>\n",
    "## 2.3.5 Geometric Interpretation of Tensor Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b613ae",
   "metadata": {},
   "source": [
    "Because the contents of __the tensors manipulated by tensor operations can be interpreted as coordinates of points in some geometric space__, all tensor operations have a geometric interpretation. \n",
    "\n",
    "For instance, let’s consider __addition__.\n",
    "\n",
    "__vector A__: `A = [0.5, 1]` is __a point in a 2D space__. \n",
    "\n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/chollet2/HighResolutionFigures/figure_2-7.png\" alt=\"2D-point in space\" style=\"width: 300px;\"/>\n",
    "\n",
    "Let’s consider a new point, `B = [1, 0.25]`, which we’ll add to the previous one. \n",
    "\n",
    "This is done geometrically by __chaining together the vector arrows__, with the __resulting location being the vector representing the sum of the previous two vectors__.\n",
    "\n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/chollet2/HighResolutionFigures/figure_2-8.png\" alt=\"vector addition\" style=\"width: 300px;\"/>\n",
    "\n",
    "As you can see, __adding a vector `B` to a vector `A`__ represents the action of __copying point `A` in a new location__, whose distance and direction from the original point `A` is determined by the vector `B`.\n",
    "\n",
    "If you apply the same vector addition to a group of points in the plane (__an “object”__), you would be creating a copy of the entire object in a new location. \n",
    "\n",
    "Thus, __tensor addition represents the action of translating an object__ (moving the object without distorting it) by a certain amount in a certain direction.\n",
    "\n",
    "In general, __elementary geometric operations__ such as translation, rotation, scaling, skewing, and so on __can be expressed as tensor operations__.\n",
    "\n",
    "* __Translation__: As you just saw, adding a vector to a point will move the point by a fixed amount in a fixed direction. Applied to a set of points (such as a 2D object), this is called a “translation”.\n",
    "\n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/02-09.png\" alt=\"translation\" style=\"width: 400px;\"/>\n",
    "\n",
    "* __Rotation__: A counterclockwise rotation of a 2D vector by an angle $\\theta$ can be achieved via a dot product with a $2 × 2$ matrix $ R = [[cos(\\theta), -sin(\\theta)], [sin(\\theta), cos(\\theta)]] $.\n",
    "\n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/chollet2/HighResolutionFigures/figure_2-10.png\" alt=\"rotation\" style=\"width: 400px;\"/>\n",
    "\n",
    "* __Scaling__: A vertical and horizontal scaling of the image can be achieved via a dot product with a $2 × 2$ matrix $S = [[horizontal_factor, 0], [0, vertical_factor]]$ (note that such a matrix is called a “__diagonal matrix__”, because it only has __non-zero coefficients in its “diagonal__”, going from the top left to the bottom right).\n",
    "\n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/chollet2/HighResolutionFigures/figure_2-11.png\" alt=\"scaling\" style=\"width: 400px;\"/>\n",
    "\n",
    "* __Linear transform__: A dot product with an arbitrary matrix implements a linear transform. Note that scaling and rotation, listed previously, are by definition linear transforms.\n",
    "\n",
    "\n",
    "* __Affine transform__: An affine transform is the __combination of a linear transform (achieved via a dot product with some matrix) and a translation (achieved via a vector addition)__. As you have probably recognized, that’s exactly the $y = W • x + b$ computation implemented by the Dense layer! A Dense layer without an activation function is an affine layer.\n",
    "\n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/chollet2/HighResolutionFigures/figure_2-12.png\" alt=\"affine_transform\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "* __Dense layer with relu activation__: An important observation about affine transforms is that if you apply many of them repeatedly, you still end up with an affine transform (so you could just have applied that one affine transform in the first place). \n",
    "\n",
    "Let’s try it with two:\n",
    "1. `affine2(affine1(x))` $= W2 • (W1 • x + b1) + b2 = (W2 • W1) • x + (W2 • b1 + b2)$ \n",
    "\n",
    "That’s an affine transform where the linear part is the matrix $W2 • W1$ and the translation part is the vector $W2 • b1 + b2$. \n",
    "\n",
    "As a consequence, a multilayer NN made entirely of Dense layers without activations would be equivalent to a single Dense layer. This “deep” neural network would just be a linear model in disguise! \n",
    "\n",
    "__This is why we need activation functions__, like __relu__. Thanks to activation functions, a chain of Dense layers can be made to implement very complex, non-linear geometric transformations, resulting in very rich hypothesis spaces for your deep NNs.\n",
    "\n",
    "<img src=\"https://drek4537l1klr.cloudfront.net/chollet2/HighResolutionFigures/figure_2-13.png\" alt=\"relu\" style=\"width: 400px;\"/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
