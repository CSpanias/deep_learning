{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a31d1ef8",
   "metadata": {},
   "source": [
    "# Deep Learning with Python (2nd ed.)\n",
    "[Website](https://www.manning.com/books/deep-learning-with-python-second-edition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3826a3",
   "metadata": {},
   "source": [
    "# 2.3 Tensor Operations\n",
    "1. [Element-Wise Operations](#elementWiseOperations)\n",
    "2. [Broadcasting](#broadcasting)\n",
    "3. [Tensor Product](#tensorProduct)\n",
    "4. [Tensor Reshaping](#tensorReshaping)\n",
    "5. [Geometric Interpretation of Tensor Operations](#geometricInterpretation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238ffc3d",
   "metadata": {},
   "source": [
    "Much as __any computer program can be ultimately reduced to a small set of binary operations on binary inputs__ (AND, OR, NOR, and so on), all __transformations learned by deep NNs can be reduced to a handful of tensor operations__ (or tensor functions) applied to tensors of numeric data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a244b2f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x18330cdc6a0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# a keras instance\n",
    "keras.layers.Dense(512, activation='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0047ce",
   "metadata": {},
   "source": [
    "This layer can be interpreted as a __function__, which __takes as input a matrix and returns another matrix__—a new representation for the input tensor. \n",
    "\n",
    "Specifically, the function is as follows (where `W` is a matrix and `b` is a vector, both attributes of the layer):\n",
    "\n",
    "`output = relu(dot(input, W) + b)`\n",
    "\n",
    "We have __three tensor operations__ here:\n",
    "\n",
    "1. A dot product (dot) between the input tensor and a tensor named `W`\n",
    "2. An addition (+) between the resulting matrix and a vector `b`\n",
    "3. A relu (rectified linear unit) operation: relu(x) is max(x, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97423519",
   "metadata": {},
   "source": [
    "<a name=\"elementWiseOperations\"></a>\n",
    "## 2.3.1 Element-Wise Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0e965a",
   "metadata": {},
   "source": [
    "The __relu operation__ and __addition__ are __element-wise operations__: operations that are __applied independently to each entry in the tensors being considered__. \n",
    "\n",
    "This means these operations are highly amenable to __massively parallel implementations__ (__vectorized implementations__). \n",
    "\n",
    "The code below demonstrates a naive Python implementation of an element-wise operation of the relu and addition operations, using a __`for` loop__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "866a3757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relu operation\n",
    "def naive_relu(x):\n",
    "    # check that x is a rank-2 tensor\n",
    "    assert len(x.shape) == 2\n",
    "    # avoid overwriting the input tensor\n",
    "    x = x.copy()\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] = max(x[i, j], 0)\n",
    "    return x\n",
    "\n",
    "# addition\n",
    "def naive_add(x, y):\n",
    "    # check that x is a rank-2 tensor\n",
    "    assert len(x.shape) == 2\n",
    "    # check that y is a rank-2 tensor\n",
    "    assert x.shape == y.shape\n",
    "    # avoid overwriting the input tensor\n",
    "    x = x.copy()\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] += y[i, j]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0f84d7",
   "metadata": {},
   "source": [
    "These operations are available as __well-optimized built-in NumPy functions__, which themselves delegate the heavy lifting to a __Basic Linear Algebra Subprograms__ (BLAS) implementation. \n",
    "\n",
    "BLAS are __low-level, highly parallel, efficient tensor-manipulation routines that are typically implemented in Fortran or C__.\n",
    "\n",
    "So, in NumPy, you can do the following element-wise operation, and it will be blazing fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36669475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "z = x + y\n",
    "z = np.maximum(z, 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fada51a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 0.01 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "  \n",
    "x = np.random.random((20, 100))\n",
    "y = np.random.random((20, 100))\n",
    "  \n",
    "t0 = time.time() \n",
    "for _ in range(1000):\n",
    "    # addition\n",
    "    z = x + y\n",
    "    # relu\n",
    "    z = np.maximum(z, 0.) \n",
    "print(\"Took: {0:.2f} s\".format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a613ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 2.81 s\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time() \n",
    "for _ in range(1000):\n",
    "    # addition\n",
    "    z = naive_add(x, y)\n",
    "    # relu\n",
    "    z = naive_relu(z) \n",
    "print(\"Took: {0:.2f} s\".format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a3adc7",
   "metadata": {},
   "source": [
    "Likewise, when running TensorFlow code on a __GPU__, element-wise operations are __executed via fully vectorized CUDA implementations__ that can best utilize the highly parallel GPU chip architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d743384",
   "metadata": {},
   "source": [
    "<a name=\"broadcasting\"></a>\n",
    "## 2.3.2 Broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc900e59",
   "metadata": {},
   "source": [
    "Our earlier naive implementation of naive_add only supports the addition of __rank-2 tensors with identical shapes__. \n",
    "\n",
    "But in the Dense layer introduced earlier, we added a __rank-2 tensor with a vector__. What happens with addition when the shapes of the two tensors being added differ?\n",
    "\n",
    "When possible, and if there’s no ambiguity, __the smaller tensor will be broadcast to match the shape of the larger tensor__. Broadcasting consists of two steps:\n",
    "\n",
    "1. Axes (called __broadcast axes__) are added to the smaller tensor to match the ndim of the larger tensor.\n",
    "2. The smaller tensor is __repeated alongside these new axes__ to match the full shape of the larger tensor.\n",
    "\n",
    "\n",
    "Consider `X` with shape `(32, 10)` and `y` with shape `(10,)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4b2f43c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 10), (10,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.random((32, 10))\n",
    "y = np.random.random((10,))\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35b2466b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add an empty first axis to y\n",
    "y = np.expand_dims(y, axis=0)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b6b8455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repeat y 32 times alongside new axis\n",
    "Y = np.concatenate([y] * 32, axis=0)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b063af17",
   "metadata": {},
   "source": [
    "In terms of implementation, __no new rank-2 tensor is created__, because that would be terribly inefficient. \n",
    "\n",
    "__The repetition operation is entirely virtual__: it happens at the algorithmic level rather than at the memory level. \n",
    "\n",
    "But thinking of the vector being repeated 10 times alongside a new axis is a helpful mental model. \n",
    "\n",
    "Here’s what a naive implementation would look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90fa1409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_add_matrix_and_vector(x, y):\n",
    "    # check that x is a rank-2 tensor\n",
    "    assert len(x.shape) == 2\n",
    "    # check that y is a vector (rank-1 tensor)\n",
    "    assert len(y.shape) == 1\n",
    "    #  # the first dim of x must be equal to the 0th dim of y\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    # avoid overwriting the input tensor\n",
    "    x = x.copy()\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] += y[j]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e08c53",
   "metadata": {},
   "source": [
    "With broadcasting, you can generally perform element-wise operations that take two inputs tensors if one tensor has shape `(a, b, ... n, n + 1, ... m)` and the other has shape `(n, n + 1, ... m)`. \n",
    "\n",
    "The broadcasting will then automatically happen for axes a through `n - 1`.\n",
    "\n",
    "The following example applies the element-wise maximum operation to two tensors of different shapes via broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f1929e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: (64, 3, 32, 10) -> rank-4 tensor\n",
      "y.shape: (32, 10) -> matrix (rank-2 tensor)\n",
      "z.shape: (64, 3, 32, 10) -> rank-4 tensor\n"
     ]
    }
   ],
   "source": [
    "x = np.random.random((64, 3, 32, 10))\n",
    "print(f\"x.shape: {x.shape} -> rank-4 tensor\")\n",
    "y = np.random.random(((32,10)))\n",
    "print(f\"y.shape: {y.shape} -> matrix (rank-2 tensor)\")\n",
    "z = np.maximum(x, y)\n",
    "print(f\"z.shape: {z.shape} -> rank-4 tensor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a04aa3",
   "metadata": {},
   "source": [
    "<a name=\"tensorProduct\"></a>\n",
    "## 2.3.3 Tensor Product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd943b6c",
   "metadata": {},
   "source": [
    "The tensor product, or __dot product__ (not to be confused with an element-wise product, the `*` operator), is __one of the most common, most useful tensor operations__.\n",
    "\n",
    "In NumPy, a tensor product is done using the `np.dot` function (because the mathematical notation for tensor product is usually a dot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d43cd864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: (32,) -> vector (rank-1 tensor)\n",
      "y.shape: (32,) -> vector (rank-1 tensor)\n",
      "z.shape: () -> scalar!\n"
     ]
    }
   ],
   "source": [
    "x = np.random.random((32,))\n",
    "print(f\"x.shape: {x.shape} -> vector (rank-1 tensor)\")\n",
    "y = np.random.random((32,))\n",
    "print(f\"y.shape: {y.shape} -> vector (rank-1 tensor)\")\n",
    "z = np.dot(x, y)\n",
    "print(f\"z.shape: {z.shape} -> scalar!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43babb6",
   "metadata": {},
   "source": [
    "Mathematically, what does the dot operation do? \n",
    "\n",
    "Let’s start with the dot product of two vectors, `x` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de154ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.824177458558538"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def naive_vector_dot(x, y):\n",
    "    # check that x is a rank-1 tensor (vector)\n",
    "    assert len(x.shape) == 1\n",
    "     # check that y is a rank-1 tensor (vector)\n",
    "    assert len(y.shape) == 1\n",
    "     # check that they have equal dims\n",
    "    assert x.shape[0] == y.shape[0]\n",
    "    z = 0.\n",
    "    for i in range(x.shape[0]):\n",
    "        z += x[i] * y[i]\n",
    "    return z\n",
    "\n",
    "naive_vector_dot(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e938959",
   "metadata": {},
   "source": [
    "You’ll have noticed that __the dot product between two vectors is a scalar__ and that only __vectors with the same number of elements__ are compatible for a dot product.\n",
    "\n",
    "You can also take the __dot product between a matrix `x` and a vector `y`__, which __returns a vector where the coefficients are the dot products between `y` and the rows of `x`__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9939914a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_matrix_vector_dot(x, y):\n",
    "    # check that x is a rank-2 tensor (matrix)\n",
    "    assert len(x.shape) == 2\n",
    "    # check that y is a rank-1 tensor (vector)\n",
    "    assert len(y.shape) == 1\n",
    "    # the first dim of x must be equal to the 0th dim of y\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    # this operation returns a vector of 0s with the same shape as y\n",
    "    z = np.zeros(x.shape[0])\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            z[i] += x[i, j] * y[j]\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8c0636df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: (32, 10) -> matrix (rank-2 tensor)\n",
      "y.shape: (10,) -> vector (rank-1 tensor)\n",
      "z.shape: (32,) -> vector (rank-1 tensor)\n"
     ]
    }
   ],
   "source": [
    "x = np.random.random((32, 10))\n",
    "print(f\"x.shape: {x.shape} -> matrix (rank-2 tensor)\")\n",
    "y = np.random.random((10,))\n",
    "print(f\"y.shape: {y.shape} -> vector (rank-1 tensor)\")\n",
    "\n",
    "z = naive_matrix_vector_dot(x, y)\n",
    "print(f\"z.shape: {z.shape} -> vector (rank-1 tensor)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c039284",
   "metadata": {},
   "source": [
    "You could also reuse the code we wrote previously, which highlights the __relationship between a matrix-vector product and a vector product__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ab60034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_matrix_vector_dot(x, y):\n",
    "    z = np.zeros(x.shape[0])\n",
    "    for i in range(x.shape[0]):\n",
    "        z[i] = naive_vector_dot(x[i, :], y)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5757b074",
   "metadata": {},
   "source": [
    "Note that as soon as one of the two tensors has an `ndim` greater than 1, __dot is no longer symmetric__, which is to say that __`dot(x, y)` isn’t the same as `dot(y, x)`__.\n",
    "\n",
    "Of course, a dot product generalizes to tensors with an arbitrary number of axes. __The most common applications may be the dot product between two matrices__. \n",
    "\n",
    "You can take the dot product of two matrices `x` and `y` `(dot(x, y))` if and only if `x.shape[1] == y.shape[0]`. \n",
    "\n",
    "The result is a matrix with shape `(x.shape[0], y.shape[1])`, where the coefficients are the vector products between the rows of `x` and the columns of `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e9cc5a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_matrix_dot(x, y):\n",
    "    # check that x is a rank-2 tensor (matrix)\n",
    "    assert len(x.shape) == 2\n",
    "    # check that y is a rank-2 tensor (matrix)\n",
    "    assert len(y.shape) == 2\n",
    "    # the first dim of x must be equal to the 0th dim of y\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    # this operation returns a matrix of 0s with a specific shape\n",
    "    z = np.zeros((x.shape[0], y.shape[1]))\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            row_x = x[i, :]\n",
    "            column_y = y[:, j]\n",
    "            z[i, j] = naive_vector_dot(row_x, column_y)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d372e53f",
   "metadata": {},
   "source": [
    "To understand __dot-product shape compatibility__, it helps to visualize the input and output tensors by aligning them as shown below.\n",
    "\n",
    "![](https://drek4537l1klr.cloudfront.net/chollet2/Figures/02-05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fabc3fa",
   "metadata": {},
   "source": [
    "In the figure, `x`, `y`, and `z` are pictured as rectangles (literal boxes of coefficients). \n",
    "\n",
    "Because __the rows of `x` and the columns of `y` must have the same size__, it follows that __the width of `x` must match the height of `y`__. \n",
    "\n",
    "If you go on to develop new machine learning algorithms, you’ll likely be drawing such diagrams often.\n",
    "\n",
    "More generally, __you can take the dot product between higher-dimensional tensors__, following the same rules for shape compatibility as outlined earlier for the 2D case.\n",
    "\n",
    "`(a, b, c, d) • (d,) → (a, b, c)` <br>\n",
    "`(a, b, c, d) • (d, e) → (a, b, c, e)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e14c265",
   "metadata": {},
   "source": [
    "<a name=\"tensorReshaping\"></a>\n",
    "## 2.3.4 Tensor Reshaping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0533d702",
   "metadata": {},
   "source": [
    "A third type of tensor operation that’s essential to understand is __tensor reshaping__. \n",
    "\n",
    "Although it wasn’t used in the Dense layers in our first NN example, __we used it when we preprocessed the digits data before feeding it into our model__.\n",
    "\n",
    "`train_images = train_images.reshape((60000, 28 * 28))`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6926a249",
   "metadata": {},
   "source": [
    "Reshaping a tensor means __rearranging its rows and columns to match a target shape__. \n",
    "\n",
    "Naturally, __the reshaped tensor has the same total number of coefficients as the initial tensor__. \n",
    "\n",
    "Reshaping is best understood via simple examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1b2e9b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: (3, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [2., 3.],\n",
       "       [4., 5.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[0., 1.],\n",
    "              [2., 3.],\n",
    "              [4., 5.]])\n",
    "              \n",
    "print(f\"x.shape: {x.shape}\")\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d95a523e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: (6, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [3.],\n",
       "       [4.],\n",
       "       [5.]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.reshape((6, 1))\n",
    "print(f\"x.shape: {x.shape}\")\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8a9c4cee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: (2, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 2.],\n",
       "       [3., 4., 5.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.reshape((2, 3))\n",
    "print(f\"x.shape: {x.shape}\")\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4e707a",
   "metadata": {},
   "source": [
    "A special case of reshaping that’s commonly encountered is __transposition__. \n",
    "\n",
    "Transposing a matrix means __exchanging its rows and its columns__, so that `x[i, :]` becomes `x[:, i]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "46fb046f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: (300, 20)\n",
      "x transposed: (20, 300)\n"
     ]
    }
   ],
   "source": [
    "x = np.zeros((300, 20))\n",
    "print(f\"x.shape: {x.shape}\")\n",
    "x = np.transpose(x)\n",
    "print(f\"x transposed: {x.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
