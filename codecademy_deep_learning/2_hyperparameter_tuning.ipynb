{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6202e665",
   "metadata": {},
   "source": [
    "1. Data Pre-Processing\n",
    "1. NN Model\n",
    "1. [Validation Set](#valset)\n",
    "1. [Manual Hyperparameter Tuning](#opt)\n",
    "    1. [`learning_rate`](#lr)\n",
    "    1. [`batch_size`](#bs)\n",
    "    1. [`epochs`](#epochs)\n",
    "    1. [Changing the Model](#layers)\n",
    "1. [Automated Hyperparameter Tuning](#auto)\n",
    "    1. [GridSearch](#gs)\n",
    "    1. [RandomSearch](#rs)\n",
    "    1. [Regularization: `Dropout`](#reg)\n",
    "    1. [Baselines](#baselines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ebf42ab1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# import data\n",
    "df = pd.read_csv('insurance.csv')\n",
    "\n",
    "# split Xs and Ys\n",
    "X = df.iloc[:, 0:6]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "# one-hot encode categorical values\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "# split train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d9c83e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import Normalizer\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# ct = ColumnTransformer([('normalize', Normalizer(), ['age', 'bmi', 'children'])], remainder='passthrough')\n",
    "\n",
    "# X_train_norm = ct.fit_transform(X_train)\n",
    "# X_test_norm = ct.transform(X_test)\n",
    "\n",
    "# X_train_norm = pd.DataFrame(X_train_norm, columns = X_train.columns)\n",
    "# X_train_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "90f09dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>sex_female</th>\n",
       "      <th>sex_male</th>\n",
       "      <th>smoker_no</th>\n",
       "      <th>smoker_yes</th>\n",
       "      <th>region_northeast</th>\n",
       "      <th>region_northwest</th>\n",
       "      <th>region_southeast</th>\n",
       "      <th>region_southwest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.265106</td>\n",
       "      <td>-0.913375</td>\n",
       "      <td>-0.912607</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.016500</td>\n",
       "      <td>0.795456</td>\n",
       "      <td>0.747689</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.405909</td>\n",
       "      <td>-0.007962</td>\n",
       "      <td>-0.082459</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.424533</td>\n",
       "      <td>0.394165</td>\n",
       "      <td>-0.912607</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.461934</td>\n",
       "      <td>1.564598</td>\n",
       "      <td>-0.912607</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age       bmi  children  sex_female  sex_male  smoker_no  smoker_yes  \\\n",
       "0  0.265106 -0.913375 -0.912607         1.0       0.0        1.0         0.0   \n",
       "1 -0.016500  0.795456  0.747689         0.0       1.0        0.0         1.0   \n",
       "2  0.405909 -0.007962 -0.082459         1.0       0.0        0.0         1.0   \n",
       "3 -1.424533  0.394165 -0.912607         1.0       0.0        1.0         0.0   \n",
       "4  1.461934  1.564598 -0.912607         0.0       1.0        0.0         1.0   \n",
       "\n",
       "   region_northeast  region_northwest  region_southeast  region_southwest  \n",
       "0               1.0               0.0               0.0               0.0  \n",
       "1               0.0               0.0               0.0               1.0  \n",
       "2               0.0               1.0               0.0               0.0  \n",
       "3               0.0               0.0               0.0               1.0  \n",
       "4               0.0               0.0               0.0               1.0  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# instantiate CT\n",
    "ct = ColumnTransformer([('standardize', StandardScaler(), ['age', 'bmi', 'children'])], remainder='passthrough')\n",
    "\n",
    "# normalize numerical vars\n",
    "X_train_scaled = ct.fit_transform(X_train)\n",
    "X_test_scaled = ct.transform(X_test)\n",
    "\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns = X_train.columns)\n",
    "X_train_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a297dcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_5 (Dense)             (None, 128)               1536      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,665\n",
      "Trainable params: 1,665\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "299/299 [==============================] - 1s 884us/step - loss: 325157600.0000 - mae: 13366.4990\n",
      "Epoch 2/50\n",
      "299/299 [==============================] - 0s 900us/step - loss: 322617120.0000 - mae: 13271.7910\n",
      "Epoch 3/50\n",
      "299/299 [==============================] - 0s 979us/step - loss: 317009856.0000 - mae: 13062.8135\n",
      "Epoch 4/50\n",
      "299/299 [==============================] - 0s 900us/step - loss: 308743712.0000 - mae: 12749.5547\n",
      "Epoch 5/50\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 298417184.0000 - mae: 12347.8730\n",
      "Epoch 6/50\n",
      "299/299 [==============================] - 0s 954us/step - loss: 286642880.0000 - mae: 11880.9766\n",
      "Epoch 7/50\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 274043744.0000 - mae: 11364.3857\n",
      "Epoch 8/50\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 260994432.0000 - mae: 10850.6152\n",
      "Epoch 9/50\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 247789104.0000 - mae: 10355.4912\n",
      "Epoch 10/50\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 234803200.0000 - mae: 9873.2949\n",
      "Epoch 11/50\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 222265168.0000 - mae: 9427.4697\n",
      "Epoch 12/50\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 210350800.0000 - mae: 9030.4277\n",
      "Epoch 13/50\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 199265920.0000 - mae: 8685.2568\n",
      "Epoch 14/50\n",
      "299/299 [==============================] - 0s 997us/step - loss: 189114896.0000 - mae: 8395.1123\n",
      "Epoch 15/50\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 179973776.0000 - mae: 8171.5977\n",
      "Epoch 16/50\n",
      "299/299 [==============================] - 0s 941us/step - loss: 171757280.0000 - mae: 8002.9204\n",
      "Epoch 17/50\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 164414736.0000 - mae: 7881.6797\n",
      "Epoch 18/50\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 157868624.0000 - mae: 7826.7881\n",
      "Epoch 19/50\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 152083488.0000 - mae: 7779.6875\n",
      "Epoch 20/50\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 146918384.0000 - mae: 7774.0537\n",
      "Epoch 21/50\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 142257728.0000 - mae: 7763.9019\n",
      "Epoch 22/50\n",
      "299/299 [==============================] - 0s 904us/step - loss: 138016112.0000 - mae: 7769.0000\n",
      "Epoch 23/50\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 134099800.0000 - mae: 7784.3296\n",
      "Epoch 24/50\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 130482104.0000 - mae: 7797.7183\n",
      "Epoch 25/50\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 127072080.0000 - mae: 7798.0244\n",
      "Epoch 26/50\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 123839144.0000 - mae: 7781.8271\n",
      "Epoch 27/50\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 120764760.0000 - mae: 7784.2607\n",
      "Epoch 28/50\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 117860288.0000 - mae: 7767.1235\n",
      "Epoch 29/50\n",
      "299/299 [==============================] - 0s 955us/step - loss: 115057744.0000 - mae: 7729.1675\n",
      "Epoch 30/50\n",
      "299/299 [==============================] - 0s 907us/step - loss: 112309776.0000 - mae: 7699.3325\n",
      "Epoch 31/50\n",
      "299/299 [==============================] - 0s 981us/step - loss: 109678256.0000 - mae: 7673.6089\n",
      "Epoch 32/50\n",
      "299/299 [==============================] - 0s 981us/step - loss: 107099080.0000 - mae: 7620.7515\n",
      "Epoch 33/50\n",
      "299/299 [==============================] - 0s 954us/step - loss: 104612696.0000 - mae: 7572.8506\n",
      "Epoch 34/50\n",
      "299/299 [==============================] - 0s 977us/step - loss: 102176072.0000 - mae: 7500.0986\n",
      "Epoch 35/50\n",
      "299/299 [==============================] - 0s 920us/step - loss: 99784344.0000 - mae: 7431.0039\n",
      "Epoch 36/50\n",
      "299/299 [==============================] - 0s 907us/step - loss: 97443408.0000 - mae: 7368.1436\n",
      "Epoch 37/50\n",
      "299/299 [==============================] - 0s 917us/step - loss: 95148552.0000 - mae: 7297.0298\n",
      "Epoch 38/50\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 92909184.0000 - mae: 7223.6577\n",
      "Epoch 39/50\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 90694920.0000 - mae: 7123.1060\n",
      "Epoch 40/50\n",
      "299/299 [==============================] - 0s 937us/step - loss: 88511624.0000 - mae: 7053.2031\n",
      "Epoch 41/50\n",
      "299/299 [==============================] - 0s 987us/step - loss: 86364752.0000 - mae: 6970.8247\n",
      "Epoch 42/50\n",
      "299/299 [==============================] - 0s 947us/step - loss: 84259464.0000 - mae: 6887.7109\n",
      "Epoch 43/50\n",
      "299/299 [==============================] - 0s 890us/step - loss: 82193592.0000 - mae: 6785.3999\n",
      "Epoch 44/50\n",
      "299/299 [==============================] - 0s 921us/step - loss: 80183600.0000 - mae: 6692.7930\n",
      "Epoch 45/50\n",
      "299/299 [==============================] - 0s 900us/step - loss: 78214288.0000 - mae: 6593.2188\n",
      "Epoch 46/50\n",
      "299/299 [==============================] - 0s 900us/step - loss: 76260928.0000 - mae: 6483.9297\n",
      "Epoch 47/50\n",
      "299/299 [==============================] - 0s 924us/step - loss: 74346512.0000 - mae: 6389.5688\n",
      "Epoch 48/50\n",
      "299/299 [==============================] - 0s 914us/step - loss: 72494016.0000 - mae: 6297.9810\n",
      "Epoch 49/50\n",
      "299/299 [==============================] - 0s 957us/step - loss: 70675448.0000 - mae: 6186.0864\n",
      "Epoch 50/50\n",
      "299/299 [==============================] - 0s 987us/step - loss: 68909032.0000 - mae: 6085.5557\n",
      "MSE: 66009356.0\n",
      "MAE: 6098.0703125\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# instantiate model\n",
    "model = Sequential(name='my_model')\n",
    "\n",
    "# instantiate & add input layer to the model\n",
    "input_layer = InputLayer(input_shape=(X.shape[1], ))\n",
    "model.add(input_layer)\n",
    "\n",
    "# instantiate & add hidden layers\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# instantiate & add output layer\n",
    "model.add(Dense(1))\n",
    "\n",
    "# check model\n",
    "print(model.summary())\n",
    "\n",
    "# compile model\n",
    "model.compile(loss='mse', metrics=['mae'], optimizer=Adam(learning_rate=0.001))\n",
    "\n",
    "# train model\n",
    "model.fit(X_train_scaled, y_train, epochs=50, batch_size=3, verbose=1)\n",
    "\n",
    "# evaluate model\n",
    "val_mse, val_mae = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(f\"MSE: {val_mse}\\nMAE: {val_mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977651a3",
   "metadata": {},
   "source": [
    "<a name=\"valset\"></a>\n",
    "# 1. Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665a817d",
   "metadata": {},
   "source": [
    "Using the training data to choose hyperparameters might lead to __overfitting__ to the training data meaning the model learns patterns specific to the training data that would not apply to new data. \n",
    "\n",
    "<img src=\"https://content.codecademy.com/courses/deeplearning-with-tensorflow/hyperparameter-tuning/hyperparameter-tuning-diagram.png\" alt=\"train_pipeline\" style=\"width: 65%;\"/>\n",
    "\n",
    "For that reason, hyperparameters are chosen on a held-out set called __validation set__. In TensorFlow Keras, validation split can be specified as a parameter in the `.fit()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75708f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 20626376.0\n",
      "MAE: 2698.529052734375\n"
     ]
    }
   ],
   "source": [
    "# compile model\n",
    "model.compile(loss='mse', metrics=['mae'], optimizer=Adam(learning_rate=0.001))\n",
    "\n",
    "# train model\n",
    "model.fit(X_train_scaled, y_train, epochs=50, batch_size=10, verbose=0, validation_split=0.2)\n",
    "\n",
    "# evaluate model\n",
    "val_mse, val_mae = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(f\"MSE: {val_mse}\\nMAE: {val_mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09b2f95",
   "metadata": {},
   "source": [
    "<a name=\"manual\"></a>\n",
    "# 2. Manual Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05245127",
   "metadata": {},
   "source": [
    "<a name='lr'></a>\n",
    "## 2.1 Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df557a1",
   "metadata": {},
   "source": [
    "A __larger__ `learning_rate` leads to a __faster learning process__ at a __cost to be stuck in a local minimum__. A __smaller__ `learning_rate` might produce a __good suboptimal or global solution__, but it will take it __much longer to converge__. In the extremes, a `learning_rate` __too large__ will lead to an __unstable learning process oscillating__ over the epochs. A `learning_rate` __too small__ may __not converge or get stuck__ in a local minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310e4365",
   "metadata": {},
   "source": [
    "<a name='bs'></a>\n",
    "## 2.2 Batch Size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9895a01e",
   "metadata": {},
   "source": [
    "The `batch_size` is a hyperparameter that determines __how many training samples are seen before updating the network’s parameters__ (weight and bias matrices).\n",
    "\n",
    "When the batch contains __all the training examples__, the process is called __batch gradient descent__. If the batch has __one sample__, it is called the __stochastic gradient descent__. And finally, when __1 < `batch_size` < number of training points__, is called __mini-batch gradient descent__. An advantage of using batches is for GPU computation that can parallelize neural network computations.\n",
    "\n",
    "A __larger__ `batch_size` will provide our model with __better gradient estimates__ and a solution close to the optimum, but this comes __at a cost of computational efficiency and good generalization__ performance. __Smaller__ `batch_size` is a __poor estimate of the gradient__, but the learning is performed __faster__. \n",
    "\n",
    "Finding the __“sweet spot”__ depends on the dataset and the problem, and can be determined through hyperparameter tuning.\n",
    "\n",
    "When using a __larger__ `batch_size` it is usually good to __increase__ `learning_rate`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf632c6",
   "metadata": {},
   "source": [
    "<a name='epochs'></a>\n",
    "## 3.3 Epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b104aa",
   "metadata": {},
   "source": [
    "`epochs` is a hyperparameter representing the number of complete passes through the training dataset. This is typically a large number (100, 1000, or larger). If the data is split into batches, __in one epoch the optimizer will see all the batches__.\n",
    "\n",
    "__Too many__ epochs can lead to __overfitting__, and __too few__ to __underfitting__. One trick is to use `EarlyStopping`: when the training performance reaches the plateau or starts degrading, the learning stops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e83b7d73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 23130380.0000 - mae: 2861.7012 - val_loss: 23635050.0000 - val_mae: 2866.0388\n",
      "Epoch 2/500\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 23126922.0000 - mae: 2860.3806 - val_loss: 23630748.0000 - val_mae: 2869.5417\n",
      "Epoch 3/500\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 23126570.0000 - mae: 2868.3662 - val_loss: 23626838.0000 - val_mae: 2871.1125\n",
      "Epoch 4/500\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 23126368.0000 - mae: 2863.5950 - val_loss: 23632732.0000 - val_mae: 2869.8965\n",
      "Epoch 5/500\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 23124372.0000 - mae: 2864.0027 - val_loss: 23634858.0000 - val_mae: 2869.8782\n",
      "Epoch 6/500\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 23122596.0000 - mae: 2859.0046 - val_loss: 23634282.0000 - val_mae: 2867.2646\n",
      "Epoch 7/500\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 23119378.0000 - mae: 2859.0769 - val_loss: 23633760.0000 - val_mae: 2866.7930\n",
      "Epoch 8/500\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 23119346.0000 - mae: 2859.6069 - val_loss: 23632628.0000 - val_mae: 2867.9348\n",
      "Epoch 9/500\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 23121574.0000 - mae: 2858.6465 - val_loss: 23634690.0000 - val_mae: 2866.4409\n",
      "Epoch 10/500\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 23128242.0000 - mae: 2869.5923 - val_loss: 23629422.0000 - val_mae: 2870.2534\n",
      "Epoch 11/500\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 23117806.0000 - mae: 2861.1094 - val_loss: 23636798.0000 - val_mae: 2864.2178\n",
      "Epoch 12/500\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 23114186.0000 - mae: 2856.6414 - val_loss: 23634750.0000 - val_mae: 2867.7026\n",
      "Epoch 13/500\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 23115752.0000 - mae: 2858.3044 - val_loss: 23638890.0000 - val_mae: 2867.9873\n",
      "Epoch 14/500\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 23116200.0000 - mae: 2859.0503 - val_loss: 23630986.0000 - val_mae: 2867.1511\n",
      "Epoch 15/500\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 23114618.0000 - mae: 2864.1479 - val_loss: 23632548.0000 - val_mae: 2871.3647\n",
      "Epoch 16/500\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 23110708.0000 - mae: 2861.1724 - val_loss: 23634044.0000 - val_mae: 2867.7881\n",
      "Epoch 17/500\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 23109390.0000 - mae: 2864.5422 - val_loss: 23630998.0000 - val_mae: 2869.1770\n",
      "Epoch 18/500\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 23108464.0000 - mae: 2864.4885 - val_loss: 23633142.0000 - val_mae: 2871.3440\n",
      "Epoch 19/500\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 23110346.0000 - mae: 2857.6162 - val_loss: 23633346.0000 - val_mae: 2867.2646\n",
      "Epoch 20/500\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 23106396.0000 - mae: 2860.1763 - val_loss: 23639058.0000 - val_mae: 2869.6780\n",
      "Epoch 21/500\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 23106706.0000 - mae: 2864.3088 - val_loss: 23637246.0000 - val_mae: 2869.6084\n",
      "Epoch 22/500\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 23104292.0000 - mae: 2861.5779 - val_loss: 23637304.0000 - val_mae: 2868.2222\n",
      "Epoch 23/500\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 23104046.0000 - mae: 2865.4954 - val_loss: 23632730.0000 - val_mae: 2872.8579\n",
      "Epoch 24/500\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 23099884.0000 - mae: 2860.9102 - val_loss: 23638392.0000 - val_mae: 2868.1602\n",
      "Epoch 25/500\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 23105234.0000 - mae: 2862.7996 - val_loss: 23633750.0000 - val_mae: 2867.6211\n",
      "Epoch 26/500\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 23102492.0000 - mae: 2860.2969 - val_loss: 23635612.0000 - val_mae: 2872.4680\n",
      "Epoch 27/500\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 23096754.0000 - mae: 2861.6885 - val_loss: 23641262.0000 - val_mae: 2869.3999\n",
      "Epoch 28/500\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 23095760.0000 - mae: 2860.9578 - val_loss: 23639224.0000 - val_mae: 2869.3040\n",
      "Epoch 29/500\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 23095384.0000 - mae: 2862.3672 - val_loss: 23639702.0000 - val_mae: 2871.2195\n",
      "Epoch 30/500\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 23098128.0000 - mae: 2855.2798 - val_loss: 23639500.0000 - val_mae: 2864.6980\n",
      "Epoch 31/500\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 23094366.0000 - mae: 2852.8955 - val_loss: 23641682.0000 - val_mae: 2864.4077\n",
      "Epoch 32/500\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 23092738.0000 - mae: 2856.3577 - val_loss: 23635262.0000 - val_mae: 2872.0708\n",
      "Epoch 33/500\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 23092462.0000 - mae: 2863.6284 - val_loss: 23632876.0000 - val_mae: 2873.8264\n",
      "Epoch 34/500\n",
      "45/45 [==============================] - 0s 3ms/step - loss: 23090352.0000 - mae: 2858.0955 - val_loss: 23640630.0000 - val_mae: 2866.7627\n",
      "Epoch 35/500\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 23088764.0000 - mae: 2859.7361 - val_loss: 23635002.0000 - val_mae: 2870.5142\n",
      "Epoch 36/500\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 23086418.0000 - mae: 2857.7222 - val_loss: 23638504.0000 - val_mae: 2866.9673\n",
      "Epoch 37/500\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 23087934.0000 - mae: 2856.8650 - val_loss: 23640166.0000 - val_mae: 2867.7603\n",
      "Epoch 38/500\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 23085630.0000 - mae: 2858.2146 - val_loss: 23637450.0000 - val_mae: 2867.8755\n",
      "Epoch 39/500\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 23088838.0000 - mae: 2853.2883 - val_loss: 23641042.0000 - val_mae: 2868.5879\n",
      "Epoch 40/500\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 23083614.0000 - mae: 2854.9666 - val_loss: 23642746.0000 - val_mae: 2866.6765\n",
      "Epoch 41/500\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 23084108.0000 - mae: 2856.5181 - val_loss: 23635590.0000 - val_mae: 2871.1577\n",
      "Epoch 42/500\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 23087436.0000 - mae: 2851.5164 - val_loss: 23639472.0000 - val_mae: 2864.6116\n",
      "Epoch 43/500\n",
      "45/45 [==============================] - 0s 2ms/step - loss: 23082350.0000 - mae: 2854.7605 - val_loss: 23643508.0000 - val_mae: 2865.5850\n",
      "Epoch 00043: early stopping\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# instantiate EarlyStopping\n",
    "es = EarlyStopping(\n",
    "    # monitor validation loss\n",
    "    monitor='val_loss',\n",
    "    # we seek minimal loss\n",
    "    mode='min', verbose=1,\n",
    "    # if plateu, continue for 40 more in case it improves after it\n",
    "    patience=40)\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, epochs=500,\n",
    "                    batch_size=16, verbose=1, validation_split=0.2,\n",
    "                    callbacks=[es])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40603a37",
   "metadata": {},
   "source": [
    "<a name='layers'></a>\n",
    "## 3.4 Chaning the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6f21bc",
   "metadata": {},
   "source": [
    "The rule of thumb is to __start with one hidden layer and add as many units as we have features in the dataset__. However, this might not always work. We need to try things out and __observe our learning curve__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9959c55f",
   "metadata": {},
   "source": [
    "<a name=\"auto\"></a>\n",
    "# 3. Automated Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8009492a",
   "metadata": {},
   "source": [
    "<a name='gs'></a>\n",
    "## 3.1 GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7703745f",
   "metadata": {},
   "source": [
    "Grid search, or exhaustive search, tries __every combination of desired hyperparameter values__. This obviously gets very __computationally demanding__ when we increase the number of values per hyperparameter or the number of hyperparameters we want to tune.\n",
    "\n",
    "To use `GridSearchCV` from `scikit-learn` for regression we need to first __wrap our NN model__ into a `KerasRegressor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b8e9d4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def design_model():\n",
    "    model = Sequential(name='my_model')\n",
    "    input = Input(shape=(X.shape[1],))\n",
    "    model.add(input)\n",
    "    # nodes = number of Xs\n",
    "    model.add(Dense(11, activation='relu'))\n",
    "    # output layer, 1 node per sample\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7f75a58e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 702us/step\n",
      "18/18 [==============================] - 0s 706us/step\n",
      "18/18 [==============================] - 0s 763us/step\n",
      "18/18 [==============================] - 0s 763us/step\n",
      "18/18 [==============================] - 0s 823us/step\n",
      "18/18 [==============================] - 0s 704us/step\n",
      "18/18 [==============================] - 0s 880us/step\n",
      "18/18 [==============================] - 0s 819us/step\n",
      "18/18 [==============================] - 0s 781us/step\n",
      "18/18 [==============================] - 0s 706us/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 998us/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 979us/step\n",
      "5/5 [==============================] - 0s 998us/step\n",
      "5/5 [==============================] - 0s 748us/step\n",
      "5/5 [==============================] - 0s 748us/step\n",
      "5/5 [==============================] - 0s 997us/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 760us/step\n"
     ]
    }
   ],
   "source": [
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from tensorflow.keras.layers import Dense, InputLayer\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "# wrap model\n",
    "model = KerasRegressor(model=design_model)\n",
    "\n",
    "# define hyperparamter values\n",
    "bs = [10, 40]\n",
    "ep = [10, 50]\n",
    "param_grid = dict(batch_size=bs, epochs=ep)\n",
    "\n",
    "# instantiate GS\n",
    "gs = GridSearchCV(estimator = model,\n",
    "                  param_grid=param_grid,\n",
    "                  scoring=make_scorer(mean_squared_error, greater_is_better=False))\n",
    "\n",
    "# extract results\n",
    "gs_res = gs.fit(X_train_scaled, y_train, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "db9f505f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KerasRegressor(\n",
       "\tmodel=<function design_model at 0x000001872AB67280>\n",
       "\tbuild_fn=None\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=10\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=50\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7471f1",
   "metadata": {},
   "source": [
    "<a name='rs'></a>\n",
    "## 3.2 RandomSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cd5d51",
   "metadata": {},
   "source": [
    "Random Search goes through __random combinations of hyperparameters__ and doesn’t try them all. Thus, we change our hyperparameter grid specification for the randomized search in order to have `more options`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "956c5f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['cv', 'error_score', 'estimator__model', 'estimator__build_fn', 'estimator__warm_start', 'estimator__random_state', 'estimator__optimizer', 'estimator__loss', 'estimator__metrics', 'estimator__batch_size', 'estimator__validation_batch_size', 'estimator__verbose', 'estimator__callbacks', 'estimator__validation_split', 'estimator__shuffle', 'estimator__run_eagerly', 'estimator__epochs', 'estimator', 'n_iter', 'n_jobs', 'param_distributions', 'pre_dispatch', 'random_state', 'refit', 'return_train_score', 'scoring', 'verbose'])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check RS parameters\n",
    "rs.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b90b5781",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 0s 956us/step\n",
      "26/26 [==============================] - 0s 718us/step\n",
      "26/26 [==============================] - 0s 836us/step\n",
      "26/26 [==============================] - 0s 839us/step\n",
      "26/26 [==============================] - 0s 722us/step\n",
      "18/18 [==============================] - 0s 760us/step\n",
      "18/18 [==============================] - 0s 821us/step\n",
      "18/18 [==============================] - 0s 1ms/step\n",
      "18/18 [==============================] - 0s 761us/step\n",
      "18/18 [==============================] - 0s 764us/step\n",
      "23/23 [==============================] - 0s 860us/step\n",
      "23/23 [==============================] - 0s 764us/step\n",
      "23/23 [==============================] - 0s 861us/step\n",
      "23/23 [==============================] - 0s 907us/step\n",
      "23/23 [==============================] - 0s 861us/step\n",
      "45/45 [==============================] - 0s 681us/step\n",
      "45/45 [==============================] - 0s 725us/step\n",
      "45/45 [==============================] - 0s 816us/step\n",
      "45/45 [==============================] - 0s 632us/step\n",
      "45/45 [==============================] - 0s 704us/step\n",
      "36/36 [==============================] - 0s 741us/step\n",
      "36/36 [==============================] - 0s 819us/step\n",
      "36/36 [==============================] - 0s 798us/step\n",
      "36/36 [==============================] - 0s 683us/step\n",
      "36/36 [==============================] - 0s 711us/step\n",
      "45/45 [==============================] - 0s 679us/step\n",
      "45/45 [==============================] - 0s 680us/step\n",
      "45/45 [==============================] - 0s 1ms/step\n",
      "45/45 [==============================] - 0s 748us/step\n",
      "45/45 [==============================] - 0s 680us/step\n",
      "20/20 [==============================] - 0s 735us/step\n",
      "20/20 [==============================] - 0s 786us/step\n",
      "20/20 [==============================] - 0s 1ms/step\n",
      "20/20 [==============================] - 0s 735us/step\n",
      "20/20 [==============================] - 0s 706us/step\n",
      "13/13 [==============================] - 0s 831us/step\n",
      "13/13 [==============================] - 0s 834us/step\n",
      "13/13 [==============================] - 0s 829us/step\n",
      "13/13 [==============================] - 0s 665us/step\n",
      "13/13 [==============================] - 0s 748us/step\n",
      "60/60 [==============================] - 0s 626us/step\n",
      "60/60 [==============================] - 0s 678us/step\n",
      "60/60 [==============================] - 0s 947us/step\n",
      "60/60 [==============================] - 0s 794us/step\n",
      "60/60 [==============================] - 0s 795us/step\n",
      "26/26 [==============================] - 0s 678us/step\n",
      "26/26 [==============================] - 0s 678us/step\n",
      "26/26 [==============================] - 0s 835us/step\n",
      "26/26 [==============================] - 0s 638us/step\n",
      "26/26 [==============================] - 0s 638us/step\n",
      "23/23 [==============================] - 0s 804us/step\n",
      "23/23 [==============================] - 0s 680us/step\n",
      "23/23 [==============================] - 0s 635us/step\n",
      "23/23 [==============================] - 0s 589us/step\n",
      "23/23 [==============================] - 0s 680us/step\n",
      "36/36 [==============================] - 0s 570us/step\n",
      "36/36 [==============================] - 0s 598us/step\n",
      "36/36 [==============================] - 0s 598us/step\n",
      "36/36 [==============================] - 0s 630us/step\n",
      "36/36 [==============================] - 0s 627us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(estimator=KerasRegressor(model=<function design_model at 0x000001872AB67280>),\n",
       "                   n_iter=12,\n",
       "                   param_distributions={'batch_size': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000001872F1ECC40>,\n",
       "                                        'epochs': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000001872DFA8B80>},\n",
       "                   scoring=make_scorer(mean_squared_error, greater_is_better=False))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "# define hyperparameter values\n",
    "param_grid = {'batch_size': sp_randint(2, 16), 'epochs': sp_randint(10, 100)}\n",
    "\n",
    "# wrap model\n",
    "model = KerasRegressor(model=design_model)\n",
    "\n",
    "# instantiate RS\n",
    "rs = RandomizedSearchCV(estimator=model,\n",
    "                          param_distributions=param_grid,\n",
    "                          scoring=make_scorer(mean_squared_error, greater_is_better=False),\n",
    "                          n_iter=12)\n",
    "\n",
    "rs.fit(X_train_scaled, y_train, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d35362d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KerasRegressor(\n",
       "\tmodel=<function design_model at 0x000001872AB67280>\n",
       "\tbuild_fn=None\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=3\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=59\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19f041f",
   "metadata": {},
   "source": [
    "<a name='reg'></a>\n",
    "## 3.3 Regularization: dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a92e936",
   "metadata": {},
   "source": [
    "Regularization is a set of techniques that __prevent the learning process to completely fit the model to the training data__ which can lead to overfitting. It makes the model simpler, smooths out the learning curve, and hence makes it more ‘regular’. \n",
    "\n",
    "There are many techniques for regularization such as __simplifying the model__, adding __weight regularization__, __weight decay__, and so on. The most common regularization method is `Dropout`.\n",
    "\n",
    "`Dropout` is a technique that __randomly ignores a number of outputs of a layer by setting them to zeros__. The __dropout rate__ is the percentage of layer outputs set to zero (usually between 20% to 50%).\n",
    "\n",
    "In Keras, we can add a dropout layer by introducing the `Dropout` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d0b6866a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "def design_model_dropout():\n",
    "    model = Sequential(name='my_first_model')\n",
    "    input = Input(shape=(X.shape[1],))\n",
    "    model.add(input)\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(32, activation='relu'))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(loss='mse', metrics=['mae'], optimizer=Adam(learning_rate=0.001))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8ef168a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 763us/step\n",
      "18/18 [==============================] - 0s 1ms/step\n",
      "18/18 [==============================] - 0s 823us/step\n",
      "18/18 [==============================] - 0s 937us/step\n",
      "18/18 [==============================] - 0s 1ms/step\n",
      "18/18 [==============================] - 0s 937us/step\n",
      "18/18 [==============================] - 0s 883us/step\n",
      "18/18 [==============================] - 0s 706us/step\n",
      "18/18 [==============================] - 0s 940us/step\n",
      "18/18 [==============================] - 0s 998us/step\n",
      "5/5 [==============================] - 0s 991us/step\n",
      "5/5 [==============================] - 0s 997us/step\n",
      "5/5 [==============================] - 0s 998us/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 991us/step\n",
      "5/5 [==============================] - 0s 997us/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "5/5 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# wrap model\n",
    "model = KerasRegressor(model=design_model_dropout)\n",
    "\n",
    "# define hyperparamter values\n",
    "bs = [10, 40]\n",
    "ep = [10, 50]\n",
    "param_grid = dict(batch_size=bs, epochs=ep)\n",
    "\n",
    "# instantiate GS\n",
    "gs = GridSearchCV(estimator = model,\n",
    "                  param_grid=param_grid,\n",
    "                  scoring=make_scorer(mean_squared_error, greater_is_better=False))\n",
    "\n",
    "# extract results\n",
    "gs_res = gs.fit(X_train_scaled, y_train, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2de485",
   "metadata": {},
   "source": [
    "<a name='baselines'></a>\n",
    "## 3.4 Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d36736a",
   "metadata": {},
   "source": [
    "A __baseline result__ is the __simplest possible prediction__ (__null accuracy__). \n",
    "\n",
    "For some problems, this may be a random result, and for others, it may be the most common class prediction. Since we are focused on a regression task, we can use averages or medians of the class distribution known as __central tendency measures__ as the result for all predictions.\n",
    "\n",
    "Scikit-learn provides `DummyRegressor`, which serves as a __baseline regression algorithm__. We’ll choose `mean` as our central tendency measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "515c2604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9190.331083088173"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# instantiate DR\n",
    "dr = DummyRegressor(strategy='mean')\n",
    "\n",
    "# fit DR\n",
    "dr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# predict on test data\n",
    "y_pred = dr.predict(X_test)\n",
    "\n",
    "# check baseline\n",
    "MAE_baseline = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "MAE_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd8eff8",
   "metadata": {},
   "source": [
    "The result of the baseline is \\$9190 and our previous model had a `val_mae` of \\$2866, so it performed much better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
